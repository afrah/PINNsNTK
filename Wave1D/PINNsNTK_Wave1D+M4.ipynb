{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7WkCgnRiYQSY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from Compute_Jacobian import jacobian # Please download 'Compute_Jacobian.py' in the repository \n",
        "import numpy as np\n",
        "import timeit\n",
        "from scipy.interpolate import griddata\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
        "import timeit\n",
        "\n",
        "import sys\n",
        "\n",
        "import scipy\n",
        "import scipy.io\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-y7cHTcJfBTR"
      },
      "outputs": [],
      "source": [
        "class Sampler:\n",
        "    # Initialize the class\n",
        "    def __init__(self, dim, coords, func, name = None):\n",
        "        self.dim = dim\n",
        "        self.coords = coords\n",
        "        self.func = func\n",
        "        self.name = name\n",
        "    def sample(self, N):\n",
        "        x = self.coords[0:1,:] + (self.coords[1:2,:]-self.coords[0:1,:])*np.random.rand(N, self.dim)\n",
        "        y = self.func(x)\n",
        "        return x, y\n",
        "\n",
        "# Define the exact solution and its derivatives\n",
        "def u(x, a, c):\n",
        "    \"\"\"\n",
        "    :param x: x = (t, x)\n",
        "    \"\"\"\n",
        "    t = x[:,0:1]\n",
        "    x = x[:,1:2]\n",
        "    return np.sin(np.pi * x) * np.cos(c * np.pi * t) + a * np.sin(2 * c * np.pi* x) * np.cos(4 * c  * np.pi * t)\n",
        "\n",
        "def u_t(x,a, c):\n",
        "    t = x[:,0:1]\n",
        "    x = x[:,1:2]\n",
        "    u_t = -  c * np.pi * np.sin(np.pi * x) * np.sin(c * np.pi * t) -  a * 4 * c * np.pi * np.sin(2 * c * np.pi* x) * np.sin(4 * c * np.pi * t)\n",
        "    return u_t\n",
        "\n",
        "def u_tt(x, a, c):\n",
        "    t = x[:,0:1]\n",
        "    x = x[:,1:2]\n",
        "    u_tt = -(c * np.pi)**2 * np.sin( np.pi * x) * np.cos(c * np.pi * t) - a * (4 * c * np.pi)**2 *  np.sin(2 * c * np.pi* x) * np.cos(4 * c * np.pi * t)\n",
        "    return u_tt\n",
        "\n",
        "def u_xx(x, a, c):\n",
        "    t = x[:,0:1]\n",
        "    x = x[:,1:2]\n",
        "    u_xx = - np.pi**2 * np.sin( np.pi * x) * np.cos(c * np.pi * t) -  a * (2 * c * np.pi)** 2 * np.sin(2 * c * np.pi* x) * np.cos(4 * c * np.pi * t)\n",
        "    return  u_xx\n",
        "\n",
        "\n",
        "def r(x, a, c):\n",
        "    return u_tt(x, a, c) - c**2 * u_xx(x, a, c)\n",
        "\n",
        "def operator(u, t, x, c, sigma_t=1.0, sigma_x=1.0):\n",
        "    u_t = tf.gradients(u, t)[0] / sigma_t\n",
        "    u_x = tf.gradients(u, x)[0] / sigma_x\n",
        "    u_tt = tf.gradients(u_t, t)[0] / sigma_t\n",
        "    u_xx = tf.gradients(u_x, x)[0] / sigma_x\n",
        "    residual = u_tt - c**2 * u_xx\n",
        "    return residual\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SDqDWN3nfSAg"
      },
      "outputs": [],
      "source": [
        "class PINN:\n",
        "    # Initialize the class\n",
        "    def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, c, kernel_size , sess):\n",
        "        # Normalization \n",
        "        X, _ = res_sampler.sample(np.int32(1e5))\n",
        "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
        "        self.mu_t, self.sigma_t = self.mu_X[0], self.sigma_X[0]\n",
        "        self.mu_x, self.sigma_x = self.mu_X[1], self.sigma_X[1]\n",
        "\n",
        "        # Samplers\n",
        "        self.operator = operator\n",
        "        self.ics_sampler = ics_sampler\n",
        "        self.bcs_sampler = bcs_sampler\n",
        "        self.res_sampler = res_sampler\n",
        "\n",
        "        self.sess = sess\n",
        "        # Initialize network weights and biases\n",
        "        self.layers = layers\n",
        "        self.weights, self.biases = self.initialize_NN(layers)\n",
        "        \n",
        "        # weights\n",
        "        self.lam_u_val = np.array(1.0)\n",
        "        self.lam_ut_val = np.array(1.0)\n",
        "        self.lam_r_val = np.array(1.0)\n",
        "      \n",
        "        # Wave constant\n",
        "        self.c = tf.constant(c, dtype=tf.float32)\n",
        "        \n",
        "        self.kernel_size = kernel_size # Size of the NTK matrix\n",
        "\n",
        "        # Define Tensorflow session\n",
        "        self.sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
        "\n",
        "        # Define placeholders and computational graph\n",
        "        self.t_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.x_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.t_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.x_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.u_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.t_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.x_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.t_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.x_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.t_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.x_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        \n",
        "        self.lam_u_tf = tf.placeholder(tf.float32, shape=self.lam_u_val.shape)\n",
        "        self.lam_ut_tf = tf.placeholder(tf.float32, shape=self.lam_u_val.shape)\n",
        "        self.lam_r_tf = tf.placeholder(tf.float32, shape=self.lam_u_val.shape)\n",
        "        \n",
        "\n",
        "        # Define placeholders for NTK computation\n",
        "        D1 = self.kernel_size    # boundary\n",
        "        D2 = self.kernel_size    # ut   \n",
        "        D3 = self.kernel_size    # residual  D1 = D3 = 3D2\n",
        "\n",
        "        self.t_u_ntk_tf = tf.placeholder(tf.float32, shape=(D1, 1))\n",
        "        self.x_u_ntk_tf = tf.placeholder(tf.float32, shape=(D1, 1))\n",
        "        \n",
        "        self.t_ut_ntk_tf = tf.placeholder(tf.float32, shape=(D2, 1))\n",
        "        self.x_ut_ntk_tf = tf.placeholder(tf.float32, shape=(D2, 1))\n",
        "        \n",
        "        self.t_r_ntk_tf = tf.placeholder(tf.float32, shape=(D3, 1))\n",
        "        self.x_r_ntk_tf = tf.placeholder(tf.float32, shape=(D3, 1))\n",
        "\n",
        "        # Evaluate predictions\n",
        "        self.u_ics_pred = self.net_u(self.t_ics_tf, self.x_ics_tf)\n",
        "        self.u_t_ics_pred = self.net_u_t(self.t_ics_tf, self.x_ics_tf)\n",
        "        self.u_bc1_pred = self.net_u(self.t_bc1_tf, self.x_bc1_tf)\n",
        "        self.u_bc2_pred = self.net_u(self.t_bc2_tf, self.x_bc2_tf)\n",
        "\n",
        "        self.u_pred = self.net_u(self.t_u_tf, self.x_u_tf)\n",
        "        self.r_pred = self.net_r(self.t_r_tf, self.x_r_tf)\n",
        "        \n",
        "        # Define predictions for NTK computation\n",
        "        self.u_ntk_pred = self.net_u(self.t_u_ntk_tf, self.x_u_ntk_tf)\n",
        "        self.ut_ntk_pred = self.net_u_t(self.t_ut_ntk_tf, self.x_ut_ntk_tf)\n",
        "        self.r_ntk_pred = self.net_r(self.t_r_ntk_tf, self.x_r_ntk_tf)\n",
        "\n",
        "        # Boundary loss and Initial loss\n",
        "        self.loss_ics_u = tf.reduce_mean(tf.square(self.u_ics_tf - self.u_ics_pred))\n",
        "        self.loss_ics_u_t = tf.reduce_mean(tf.square(self.u_t_ics_pred))\n",
        "        self.loss_bc1 = tf.reduce_mean(tf.square(self.u_bc1_pred))\n",
        "        self.loss_bc2 = tf.reduce_mean(tf.square(self.u_bc2_pred))\n",
        "\n",
        "        self.loss_bcs = self.loss_ics_u + self.loss_bc1 + self.loss_bc2\n",
        "\n",
        "        # Residual loss\n",
        "        self.loss_res = tf.reduce_mean(tf.square(self.r_pred))\n",
        "\n",
        "        # Total loss\n",
        "        self.loss = self.lam_r_tf * self.loss_res + self.lam_u_tf * self.loss_bcs + self.lam_ut_tf * self.loss_ics_u_t \n",
        "\n",
        "        # Define optimizer with learning rate schedule\n",
        "        self.global_step = tf.Variable(0, trainable=False)\n",
        "        starter_learning_rate = 1e-3\n",
        "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step,\n",
        "                                                        1000, 0.9, staircase=False)\n",
        "        # Passing global_step to minimize() will increment it at each step.\n",
        "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
        "\n",
        "        # Logger\n",
        "        self.loss_bcs_log = []\n",
        "        self.loss_ut_ics_log = []\n",
        "        self.loss_res_log = []\n",
        "        self.saver = tf.train.Saver()\n",
        "        \n",
        "        # Compute the Jacobian for weights and biases in each hidden layer  \n",
        "        self.J_u = self.compute_jacobian(self.u_ntk_pred)\n",
        "        self.J_ut = self.compute_jacobian(self.ut_ntk_pred)\n",
        "        self.J_r = self.compute_jacobian(self.r_ntk_pred)\n",
        "        \n",
        "        self.K_u = self.compute_ntk(self.J_u, D1, self.J_u, D1)\n",
        "        self.K_ut = self.compute_ntk(self.J_ut, D2, self.J_ut, D2)\n",
        "        self.K_r = self.compute_ntk(self.J_r, D3, self.J_r, D3)\n",
        "        \n",
        "        # NTK logger \n",
        "        self.K_u_log = []\n",
        "        self.K_ut_log = []\n",
        "        self.K_r_log = []\n",
        "        \n",
        "        # weights logger\n",
        "        self.lam_u_log = []\n",
        "        self.lam_ut_log = []\n",
        "        self.lam_r_log = []\n",
        "        \n",
        "         # Initialize Tensorflow variables\n",
        "        init = tf.global_variables_initializer()\n",
        "        self.sess.run(init)\n",
        "\n",
        "    # Initialize network weights and biases using Xavier initialization\n",
        "    def initialize_NN(self, layers):\n",
        "        # Xavier initialization\n",
        "        def xavier_init(size):\n",
        "            in_dim = size[0]\n",
        "            out_dim = size[1]\n",
        "            xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
        "            return tf.Variable(tf.random.normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev,\n",
        "                               dtype=tf.float32)\n",
        "\n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0, num_layers - 1):\n",
        "            W = xavier_init(size=[layers[l], layers[l + 1]])\n",
        "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
        "            weights.append(W)\n",
        "            biases.append(b)\n",
        "        return weights, biases\n",
        "\n",
        "    # Evaluates the forward pass\n",
        "    def forward_pass(self, H, layers, weights, biases):\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0, num_layers - 2):\n",
        "            W = weights[l]\n",
        "            b = biases[l]\n",
        "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
        "        W = weights[-1]\n",
        "        b = biases[-1]\n",
        "        H = tf.add(tf.matmul(H, W), b)\n",
        "        return H\n",
        "\n",
        "    # Forward pass for u\n",
        "    def net_u(self, t, x):\n",
        "        u = self.forward_pass(tf.concat([t, x], 1),\n",
        "                              self.layers,\n",
        "                              self.weights,\n",
        "                              self.biases)\n",
        "        return u\n",
        "\n",
        "    # Forward pass for du/dt\n",
        "    def net_u_t(self, t, x):\n",
        "        u_t = tf.gradients(self.net_u(t, x), t)[0] / self.sigma_t\n",
        "        return u_t\n",
        "\n",
        "    # Forward pass for the residual\n",
        "    def net_r(self, t, x):\n",
        "        u = self.net_u(t, x)\n",
        "        residual = self.operator(u, t, x,\n",
        "                                 self.c,\n",
        "                                 self.sigma_t,\n",
        "                                 self.sigma_x)\n",
        "        return residual\n",
        "    \n",
        "    \n",
        "    # Compute Jacobian for each weights and biases in each layer and retrun a list \n",
        "    def compute_jacobian(self, f):\n",
        "        J_list =[]\n",
        "        L = len(self.weights)    \n",
        "        for i in range(L):\n",
        "            J_w = jacobian(f, self.weights[i])\n",
        "            J_list.append(J_w)\n",
        "     \n",
        "        for i in range(L):\n",
        "            J_b = jacobian(f, self.biases[i])\n",
        "            J_list.append(J_b)\n",
        "        return J_list\n",
        "    \n",
        "    # Compute the empirical NTK = J J^T\n",
        "    def compute_ntk(self, J1_list, D1, J2_list, D2):\n",
        "\n",
        "        N = len(J1_list)\n",
        "        \n",
        "        Ker = tf.zeros((D1,D2))\n",
        "        for k in range(N):\n",
        "            J1 = tf.reshape(J1_list[k], shape=(D1,-1))\n",
        "            J2 = tf.reshape(J2_list[k], shape=(D2,-1))\n",
        "            \n",
        "            K = tf.matmul(J1, tf.transpose(J2))\n",
        "            Ker = Ker + K\n",
        "        return Ker\n",
        "\n",
        "    def fetch_minibatch(self, sampler, N):\n",
        "        X, Y = sampler.sample(N)\n",
        "        X = (X - self.mu_X) / self.sigma_X\n",
        "        return X, Y\n",
        "\n",
        "        # Trains the model by minimizing the MSE loss\n",
        "\n",
        "    def trainmb(self, nIter=10000, batch_size=128, log_NTK=False, update_lam=False):\n",
        "\n",
        "        start_time = timeit.default_timer()\n",
        "        for it in range(nIter):\n",
        "            # Fetch boundary mini-batches\n",
        "            X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, batch_size // 3)\n",
        "            X_bc1_batch, _ = self.fetch_minibatch(self.bcs_sampler[0], batch_size // 3)\n",
        "            X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], batch_size // 3)\n",
        "            \n",
        "            # Fetch residual mini-batch\n",
        "            X_res_batch, _ = self.fetch_minibatch(self.res_sampler, batch_size)\n",
        "            # Define a dictionary for associating placeholders with data\n",
        "            tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1], self.x_ics_tf: X_ics_batch[:, 1:2],\n",
        "                       self.u_ics_tf: u_ics_batch,\n",
        "                       self.t_bc1_tf: X_bc1_batch[:, 0:1], self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
        "                       self.t_bc2_tf: X_bc2_batch[:, 0:1], self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
        "                       self.t_r_tf: X_res_batch[:, 0:1], self.x_r_tf: X_res_batch[:, 1:2],\n",
        "                       self.lam_u_tf: self.lam_u_val,\n",
        "                       self.lam_ut_tf: self.lam_ut_val,\n",
        "                       self.lam_r_tf: self.lam_r_val}\n",
        "\n",
        "            # Run the Tensorflow session to minimize the loss\n",
        "            self.sess.run(self.train_op, tf_dict)\n",
        "\n",
        "            # Print\n",
        "            if it % 100 == 0:\n",
        "                elapsed = timeit.default_timer() - start_time\n",
        "\n",
        "                loss_value = self.sess.run(self.loss, tf_dict)\n",
        "                loss_bcs_value = self.sess.run(self.loss_bcs, tf_dict)\n",
        "                loss_ics_ut_value = self.sess.run(self.loss_ics_u_t, tf_dict)\n",
        "                loss_res_value = self.sess.run(self.loss_res, tf_dict)\n",
        "\n",
        "                # # Store losses\n",
        "                # self.loss_bcs_log.append(loss_bcs_value)\n",
        "                # self.loss_res_log.append(loss_res_value)\n",
        "                # self.loss_ut_ics_log.append(loss_ics_ut_value)\n",
        "\n",
        "                print('It: %d, Loss: %.3e, Loss_res: %.3e,  Loss_bcs: %.3e, Loss_ut_ics: %.3e,, Time: %.2f' %(it, loss_value, loss_res_value, loss_bcs_value, loss_ics_ut_value, elapsed))\n",
        "                \n",
        " \n",
        "                sys.stdout.flush()\n",
        "                start_time = timeit.default_timer()\n",
        "            \n",
        "          \n",
        "                if it % 100 == 0: \n",
        " \n",
        "                        lam_u_val, lam_ut_val, lam_r_val =  self.sess.run([self.loss_bcs, self.loss_ics_u_t, self.loss_res], tf_dict)\n",
        "                \n",
        "                        self.lam_u_val = lam_u_val\n",
        "                        self.lam_ut_val = lam_ut_val\n",
        "                        self.lam_r_val = lam_r_val\n",
        "                        print('lambda_u: {:.3e}'.format(self.lam_u_val))\n",
        "                        print('lambda_ut: {:.3e}'.format(self.lam_ut_val))\n",
        "                        print('lambda_r: {:.3e}'.format(self.lam_r_val))\n",
        "                        \n",
        "    def train(self, nIter , bcbatch_size , ubatch_size):\n",
        "\n",
        "        start_time = timeit.default_timer()\n",
        "\n",
        "        # Fetch boundary mini-batches\n",
        "        X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, bcbatch_size)\n",
        "        X_bc1_batch, _ = self.fetch_minibatch(self.bcs_sampler[0], bcbatch_size )\n",
        "        X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], bcbatch_size )\n",
        "        \n",
        "        # Fetch residual mini-batch\n",
        "        X_res_batch, _ = self.fetch_minibatch(self.res_sampler, ubatch_size)\n",
        "        # print(\"inside trainmb: \" , X_res_batch.shape)\n",
        "        # Define a dictionary for associating placeholders with data\n",
        "        tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1], self.x_ics_tf: X_ics_batch[:, 1:2],\n",
        "                    self.u_ics_tf: u_ics_batch,\n",
        "                    self.t_bc1_tf: X_bc1_batch[:, 0:1], self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
        "                    self.t_bc2_tf: X_bc2_batch[:, 0:1], self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
        "                    self.t_r_tf: X_res_batch[:, 0:1], self.x_r_tf: X_res_batch[:, 1:2],\n",
        "                    self.lam_u_tf: self.lam_u_val,\n",
        "                    self.lam_ut_tf: self.lam_ut_val,\n",
        "                    self.lam_r_tf: self.lam_r_val\n",
        "                    }\n",
        "        \n",
        "        # Fetch boundary mini-batches\n",
        "        X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, self.kernel_size // 3)\n",
        "        X_bc1_batch, _ = self.fetch_minibatch(self.bcs_sampler[0], self.kernel_size // 3)\n",
        "        X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], self.kernel_size // 3)\n",
        "\n",
        "        X_bc_batch = np.vstack([X_ics_batch, X_bc1_batch, X_bc2_batch])\n",
        "        X_res_batch, _ = self.fetch_minibatch(self.res_sampler, self.kernel_size)\n",
        "        X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, self.kernel_size )\n",
        "\n",
        "        ntk_dict = {self.t_u_ntk_tf: X_bc_batch[:,0:1], self.x_u_ntk_tf: X_bc_batch[:, 1:2],\n",
        "                                   self.t_ut_ntk_tf: X_ics_batch[:,0:1], self.x_ut_ntk_tf: X_ics_batch[:, 1:2],\n",
        "                                   self.t_r_ntk_tf: X_res_batch[:,0:1], self.x_r_ntk_tf: X_res_batch[:, 1:2]}\n",
        "        for it in range(nIter):\n",
        "\n",
        "            # Run the Tensorflow session to minimize the loss\n",
        "            self.sess.run(self.train_op, tf_dict)\n",
        "\n",
        "            # Print\n",
        "            if it % 100 == 0:\n",
        "                elapsed = timeit.default_timer() - start_time\n",
        "\n",
        "                loss_value = self.sess.run(self.loss, tf_dict)\n",
        "                loss_bcs_value = self.sess.run(self.loss_bcs, tf_dict)\n",
        "                loss_ics_ut_value = self.sess.run(self.loss_ics_u_t, tf_dict)\n",
        "                loss_res_value = self.sess.run(self.loss_res, tf_dict)\n",
        "\n",
        "                # # Store losses\n",
        "                # self.loss_bcs_log.append(loss_bcs_value)\n",
        "                # self.loss_res_log.append(loss_res_value)\n",
        "                # self.loss_ut_ics_log.append(loss_ics_ut_value)\n",
        "\n",
        "                print('It: %d, Loss: %.3e, Loss_res: %.3e,  Loss_bcs: %.3e, Loss_ut_ics: %.3e,, Time: %.2f' %(it, loss_value, loss_res_value, loss_bcs_value, loss_ics_ut_value, elapsed))\n",
        "                \n",
        "                print('lambda_u: {:.3e}'.format(self.lam_u_val))\n",
        "                print('lambda_ut: {:.3e}'.format(self.lam_ut_val))\n",
        "                print('lambda_r: {:.3e}'.format(self.lam_r_val))\n",
        "                sys.stdout.flush()\n",
        "                start_time = timeit.default_timer()\n",
        "            \n",
        "          \n",
        "                if it % 100 == 0:\n",
        "                        print(\"Compute NTK...\")\n",
        "\n",
        "                    # # Fetch boundary mini-batches\n",
        "                    #     X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, self.kernel_size // 3)\n",
        "                    #     X_bc1_batch, _ = self.fetch_minibatch(self.bcs_sampler[0], self.kernel_size // 3)\n",
        "                    #     X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], self.kernel_size // 3)\n",
        "                        \n",
        "                        # Fetch residual mini-batch\n",
        "                \n",
        "                        \n",
        "                        K_u_value, K_ut_value, K_r_value =  self.sess.run([self.K_u, self.K_ut, self.K_r], ntk_dict)\n",
        "                        \n",
        "                        trace_K = np.trace(K_u_value) + np.trace(K_ut_value) +  np.trace(K_r_value)\n",
        "\n",
        "                        # # Store NTK matrices\n",
        "                        # self.K_u_log.append(K_u_value)\n",
        "                        # self.K_ut_log.append(K_ut_value)\n",
        "                        # self.K_r_log.append(K_r_value)\n",
        "                        \n",
        "                        # if update_lam:\n",
        "\n",
        "                        self.lam_u_val = trace_K / np.trace(K_u_value)\n",
        "                        self.lam_ut_val = trace_K /np.trace(K_ut_value)\n",
        "                        self.lam_r_val = trace_K / np.trace(K_r_value)\n",
        "\n",
        "                        #   # Store NTK weights\n",
        "                        #   self.lam_u_log.append(self.lam_u_val)\n",
        "                        #   self.lam_ut_log.append(self.lam_ut_val)\n",
        "                        #   self.lam_r_log.append(self.lam_r_val)\n",
        "                        \n",
        "\n",
        "\n",
        "    # Evaluates predictions at test points\n",
        "    def predict_u(self, X_star):\n",
        "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
        "        tf_dict = {self.t_u_tf: X_star[:, 0:1], self.x_u_tf: X_star[:, 1:2]}\n",
        "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
        "        return u_star\n",
        "\n",
        "        # Evaluates predictions at test points\n",
        "\n",
        "    def predict_r(self, X_star):\n",
        "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
        "        tf_dict = {self.t_r_tf: X_star[:, 0:1], self.x_r_tf: X_star[:, 1:2]}\n",
        "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
        "        return r_star\n",
        "    \n",
        "   ###############################################################################################################################################\n",
        "   # \n",
        "   # ###############################################################################################################################################\n",
        "   # \n",
        "   # ###############################################################################################################################################\n",
        "   # \n",
        "   #  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#test_method(mtd , layers,  X_u, Y_u, X_r, Y_r ,  X_star , u_star , r_star  , nIter ,batch_size , bcbatch_size , ubatch_size)\n",
        "def test_method(method , layers,  ics_sampler, bcs_sampler, res_sampler, c ,kernel_size , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size ):\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
        "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
        "        # sess.run(init)\n",
        "\n",
        "        model = PINN(layers, operator, ics_sampler, bcs_sampler, res_sampler, c, kernel_size , sess)\n",
        "        # Train model\n",
        "        start_time = time.time()\n",
        "\n",
        "        if method ==\"full_batch\":\n",
        "            print(\"full_batch method is used\")\n",
        "            model.train(nIter  , bcbatch_size , ubatch_size  )\n",
        "        elif method ==\"mini_batch\":\n",
        "            print(\"mini_batch method is used\")\n",
        "            model.trainmb(nIter, mbbatch_size)\n",
        "        else:\n",
        "            print(\"unknown method!\")\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        # Predictions\n",
        "        u_pred = model.predict_u(X_star)\n",
        "        r_pred = model.predict_r(X_star)\n",
        "        # Predictions\n",
        "\n",
        "        sess.close()   \n",
        "\n",
        "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "\n",
        "    print('elapsed: {:.2e}'.format(elapsed))\n",
        "\n",
        "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "\n",
        "\n",
        "    return [elapsed, error_u  ]\n",
        "\n",
        "###############################################################################################################################################\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YGFobW0EatXj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Method:  mini_batch\n",
            "Epoch:  1\n",
            "WARNING:tensorflow:From /tmp/ipykernel_38237/187544903.py:4: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_38237/187544903.py:5: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_38237/187544903.py:6: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_38237/187544903.py:6: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tmp/ipykernel_38237/1679846595.py:35: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-24 18:21:19.297175: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
            "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-24 18:21:19.323812: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
            "2023-11-24 18:21:19.324250: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ab39dc88b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2023-11-24 18:21:19.324266: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2023-11-24 18:21:19.324737: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tmp/ipykernel_38237/1679846595.py:101: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_38237/1679846595.py:104: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_38237/1679846595.py:110: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_38237/1679846595.py:132: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "mini_batch method is used\n",
            "It: 0, Loss: 2.350e+01, Loss_res: 2.102e+00,  Loss_bcs: 6.443e+00, Loss_ut_ics: 1.495e+01,, Time: 1.77\n",
            "lambda_u: 1.000e+00\n",
            "lambda_ut: 1.000e+00\n",
            "lambda_r: 1.000e+00\n",
            "Compute NTK...\n",
            "It: 100, Loss: 2.493e+00, Loss_res: 1.580e-03,  Loss_bcs: 3.555e-01, Loss_ut_ics: 6.479e-02,, Time: 11.62\n",
            "lambda_u: 6.503e+00\n",
            "lambda_ut: 2.735e+00\n",
            "lambda_r: 2.081e+00\n",
            "Compute NTK...\n",
            "It: 200, Loss: 2.101e+00, Loss_res: 7.867e-03,  Loss_bcs: 3.982e-01, Loss_ut_ics: 3.669e-02,, Time: 9.17\n",
            "lambda_u: 5.024e+00\n",
            "lambda_ut: 2.051e+00\n",
            "lambda_r: 3.191e+00\n",
            "Compute NTK...\n",
            "It: 300, Loss: 3.601e+00, Loss_res: 3.569e-02,  Loss_bcs: 3.524e-01, Loss_ut_ics: 9.641e-03,, Time: 9.37\n",
            "lambda_u: 9.959e+00\n",
            "lambda_ut: 3.412e+00\n",
            "lambda_r: 1.649e+00\n",
            "Compute NTK...\n",
            "It: 400, Loss: 4.372e+00, Loss_res: 1.073e-01,  Loss_bcs: 2.380e-01, Loss_ut_ics: 5.350e-02,, Time: 9.81\n",
            "lambda_u: 1.661e+01\n",
            "lambda_ut: 5.139e+00\n",
            "lambda_r: 1.342e+00\n",
            "Compute NTK...\n",
            "It: 500, Loss: 1.472e+01, Loss_res: 4.242e-01,  Loss_bcs: 2.326e-01, Loss_ut_ics: 1.311e-01,, Time: 9.92\n",
            "lambda_u: 5.563e+01\n",
            "lambda_ut: 9.919e+00\n",
            "lambda_r: 1.135e+00\n",
            "Compute NTK...\n",
            "It: 600, Loss: 1.035e+01, Loss_res: 2.447e-01,  Loss_bcs: 1.449e-01, Loss_ut_ics: 1.819e-02,, Time: 9.84\n",
            "lambda_u: 6.806e+01\n",
            "lambda_ut: 1.171e+01\n",
            "lambda_r: 1.111e+00\n",
            "Compute NTK...\n",
            "It: 700, Loss: 1.677e+01, Loss_res: 7.796e-01,  Loss_bcs: 1.501e-01, Loss_ut_ics: 1.021e-02,, Time: 9.95\n",
            "lambda_u: 1.051e+02\n",
            "lambda_ut: 1.420e+01\n",
            "lambda_r: 1.087e+00\n",
            "Compute NTK...\n",
            "It: 800, Loss: 2.249e+01, Loss_res: 1.375e+00,  Loss_bcs: 1.344e-01, Loss_ut_ics: 9.380e-02,, Time: 10.07\n",
            "lambda_u: 1.464e+02\n",
            "lambda_ut: 1.409e+01\n",
            "lambda_r: 1.084e+00\n",
            "Compute NTK...\n",
            "It: 900, Loss: 2.599e+01, Loss_res: 1.344e+00,  Loss_bcs: 1.294e-01, Loss_ut_ics: 1.337e-01,, Time: 9.98\n",
            "lambda_u: 1.762e+02\n",
            "lambda_ut: 1.295e+01\n",
            "lambda_r: 1.090e+00\n",
            "Compute NTK...\n",
            "It: 1000, Loss: 2.514e+01, Loss_res: 8.646e-01,  Loss_bcs: 1.052e-01, Loss_ut_ics: 6.533e-02,, Time: 10.00\n",
            "lambda_u: 2.209e+02\n",
            "lambda_ut: 1.468e+01\n",
            "lambda_r: 1.078e+00\n",
            "Compute NTK...\n",
            "It: 1100, Loss: 2.947e+01, Loss_res: 1.352e+00,  Loss_bcs: 1.014e-01, Loss_ut_ics: 7.971e-02,, Time: 10.07\n",
            "lambda_u: 2.644e+02\n",
            "lambda_ut: 1.512e+01\n",
            "lambda_r: 1.075e+00\n",
            "Compute NTK...\n",
            "It: 1200, Loss: 2.871e+01, Loss_res: 1.668e+00,  Loss_bcs: 8.889e-02, Loss_ut_ics: 1.248e-01,, Time: 10.62\n",
            "lambda_u: 2.847e+02\n",
            "lambda_ut: 1.269e+01\n",
            "lambda_r: 1.090e+00\n",
            "Compute NTK...\n",
            "It: 1300, Loss: 3.167e+01, Loss_res: 2.125e+00,  Loss_bcs: 7.708e-02, Loss_ut_ics: 1.395e-01,, Time: 10.31\n",
            "lambda_u: 3.584e+02\n",
            "lambda_ut: 1.237e+01\n",
            "lambda_r: 1.091e+00\n",
            "Compute NTK...\n",
            "It: 1400, Loss: 5.698e+01, Loss_res: 3.792e+00,  Loss_bcs: 8.939e-02, Loss_ut_ics: 8.758e-01,, Time: 10.34\n",
            "lambda_u: 4.887e+02\n",
            "lambda_ut: 1.037e+01\n",
            "lambda_r: 1.109e+00\n",
            "Compute NTK...\n",
            "It: 1500, Loss: 4.913e+01, Loss_res: 2.920e+00,  Loss_bcs: 7.526e-02, Loss_ut_ics: 2.110e-01,, Time: 10.21\n",
            "lambda_u: 5.784e+02\n",
            "lambda_ut: 1.138e+01\n",
            "lambda_r: 1.098e+00\n",
            "Compute NTK...\n",
            "It: 1600, Loss: 6.355e+01, Loss_res: 6.241e+00,  Loss_bcs: 7.026e-02, Loss_ut_ics: 4.411e-01,, Time: 10.23\n",
            "lambda_u: 7.378e+02\n",
            "lambda_ut: 1.095e+01\n",
            "lambda_r: 1.102e+00\n",
            "Compute NTK...\n",
            "It: 1700, Loss: 6.527e+01, Loss_res: 5.695e+00,  Loss_bcs: 5.863e-02, Loss_ut_ics: 4.183e-01,, Time: 10.24\n",
            "lambda_u: 9.258e+02\n",
            "lambda_ut: 1.131e+01\n",
            "lambda_r: 1.098e+00\n",
            "Compute NTK...\n",
            "It: 1800, Loss: 8.535e+01, Loss_res: 7.748e+00,  Loss_bcs: 5.959e-02, Loss_ut_ics: 4.762e-01,, Time: 10.22\n",
            "lambda_u: 1.197e+03\n",
            "lambda_ut: 1.157e+01\n",
            "lambda_r: 1.096e+00\n",
            "Compute NTK...\n",
            "It: 1900, Loss: 7.266e+01, Loss_res: 4.480e+00,  Loss_bcs: 4.869e-02, Loss_ut_ics: 9.785e-01,, Time: 10.32\n",
            "lambda_u: 1.189e+03\n",
            "lambda_ut: 1.002e+01\n",
            "lambda_r: 1.112e+00\n",
            "Compute NTK...\n",
            "It: 2000, Loss: 7.033e+01, Loss_res: 4.693e+00,  Loss_bcs: 4.879e-02, Loss_ut_ics: 9.263e-01,, Time: 10.20\n",
            "lambda_u: 1.143e+03\n",
            "lambda_ut: 1.008e+01\n",
            "lambda_r: 1.111e+00\n",
            "Compute NTK...\n",
            "It: 2100, Loss: 6.555e+01, Loss_res: 5.924e+00,  Loss_bcs: 3.875e-02, Loss_ut_ics: 5.697e-01,, Time: 10.14\n",
            "lambda_u: 1.356e+03\n",
            "lambda_ut: 1.142e+01\n",
            "lambda_r: 1.097e+00\n",
            "Compute NTK...\n",
            "It: 2200, Loss: 6.645e+01, Loss_res: 9.250e+00,  Loss_bcs: 2.945e-02, Loss_ut_ics: 6.852e-01,, Time: 10.19\n",
            "lambda_u: 1.580e+03\n",
            "lambda_ut: 1.456e+01\n",
            "lambda_r: 1.074e+00\n",
            "Compute NTK...\n",
            "It: 2300, Loss: 6.552e+01, Loss_res: 7.949e+00,  Loss_bcs: 3.165e-02, Loss_ut_ics: 3.464e-01,, Time: 10.24\n",
            "lambda_u: 1.636e+03\n",
            "lambda_ut: 1.511e+01\n",
            "lambda_r: 1.072e+00\n",
            "Compute NTK...\n",
            "It: 2400, Loss: 5.487e+01, Loss_res: 7.492e+00,  Loss_bcs: 2.487e-02, Loss_ut_ics: 3.793e-01,, Time: 10.14\n",
            "lambda_u: 1.699e+03\n",
            "lambda_ut: 1.166e+01\n",
            "lambda_r: 1.094e+00\n",
            "Compute NTK...\n",
            "It: 2500, Loss: 4.058e+01, Loss_res: 4.501e+00,  Loss_bcs: 1.699e-02, Loss_ut_ics: 3.295e-01,, Time: 10.47\n",
            "lambda_u: 1.862e+03\n",
            "lambda_ut: 1.231e+01\n",
            "lambda_r: 1.089e+00\n",
            "Compute NTK...\n",
            "It: 2600, Loss: 3.998e+01, Loss_res: 5.927e+00,  Loss_bcs: 1.510e-02, Loss_ut_ics: 1.524e-01,, Time: 10.28\n",
            "lambda_u: 2.094e+03\n",
            "lambda_ut: 1.274e+01\n",
            "lambda_r: 1.086e+00\n",
            "Compute NTK...\n",
            "It: 2700, Loss: 4.190e+01, Loss_res: 4.820e+00,  Loss_bcs: 1.409e-02, Loss_ut_ics: 1.756e-01,, Time: 10.17\n",
            "lambda_u: 2.436e+03\n",
            "lambda_ut: 1.338e+01\n",
            "lambda_r: 1.081e+00\n",
            "Compute NTK...\n",
            "It: 2800, Loss: 3.761e+01, Loss_res: 6.443e+00,  Loss_bcs: 1.136e-02, Loss_ut_ics: 1.215e-01,, Time: 10.42\n",
            "lambda_u: 2.546e+03\n",
            "lambda_ut: 1.444e+01\n",
            "lambda_r: 1.075e+00\n",
            "Compute NTK...\n",
            "It: 2900, Loss: 3.306e+01, Loss_res: 6.122e+00,  Loss_bcs: 9.456e-03, Loss_ut_ics: 9.017e-02,, Time: 10.55\n",
            "lambda_u: 2.666e+03\n",
            "lambda_ut: 1.390e+01\n",
            "lambda_r: 1.078e+00\n",
            "Compute NTK...\n",
            "It: 3000, Loss: 4.336e+01, Loss_res: 5.816e+00,  Loss_bcs: 1.166e-02, Loss_ut_ics: 3.745e-01,, Time: 12.13\n",
            "lambda_u: 2.699e+03\n",
            "lambda_ut: 1.516e+01\n",
            "lambda_r: 1.071e+00\n",
            "Compute NTK...\n",
            "It: 3100, Loss: 2.770e+01, Loss_res: 5.738e+00,  Loss_bcs: 7.085e-03, Loss_ut_ics: 4.989e-02,, Time: 10.12\n",
            "lambda_u: 2.937e+03\n",
            "lambda_ut: 1.490e+01\n",
            "lambda_r: 1.072e+00\n",
            "Compute NTK...\n",
            "It: 3200, Loss: 2.945e+01, Loss_res: 5.805e+00,  Loss_bcs: 7.068e-03, Loss_ut_ics: 6.467e-02,, Time: 9.97\n",
            "lambda_u: 3.141e+03\n",
            "lambda_ut: 1.659e+01\n",
            "lambda_r: 1.065e+00\n",
            "Compute NTK...\n",
            "It: 3300, Loss: 3.794e+01, Loss_res: 1.117e+01,  Loss_bcs: 7.358e-03, Loss_ut_ics: 1.940e-01,, Time: 10.08\n",
            "lambda_u: 3.123e+03\n",
            "lambda_ut: 1.554e+01\n",
            "lambda_r: 1.069e+00\n",
            "Compute NTK...\n",
            "It: 3400, Loss: 3.271e+01, Loss_res: 8.582e+00,  Loss_bcs: 5.410e-03, Loss_ut_ics: 1.823e-01,, Time: 10.03\n",
            "lambda_u: 3.783e+03\n",
            "lambda_ut: 1.715e+01\n",
            "lambda_r: 1.062e+00\n",
            "Compute NTK...\n",
            "It: 3500, Loss: 2.686e+01, Loss_res: 6.828e+00,  Loss_bcs: 4.952e-03, Loss_ut_ics: 2.627e-02,, Time: 9.70\n",
            "lambda_u: 3.868e+03\n",
            "lambda_ut: 1.990e+01\n",
            "lambda_r: 1.053e+00\n",
            "Compute NTK...\n",
            "It: 3600, Loss: 3.549e+01, Loss_res: 1.197e+01,  Loss_bcs: 4.625e-03, Loss_ut_ics: 8.323e-02,, Time: 9.64\n",
            "lambda_u: 4.542e+03\n",
            "lambda_ut: 2.385e+01\n",
            "lambda_r: 1.044e+00\n",
            "Compute NTK...\n",
            "It: 3700, Loss: 2.335e+01, Loss_res: 6.223e+00,  Loss_bcs: 3.433e-03, Loss_ut_ics: 7.396e-02,, Time: 9.52\n",
            "lambda_u: 4.420e+03\n",
            "lambda_ut: 2.245e+01\n",
            "lambda_r: 1.047e+00\n",
            "Compute NTK...\n",
            "It: 3800, Loss: 2.364e+01, Loss_res: 6.498e+00,  Loss_bcs: 3.251e-03, Loss_ut_ics: 4.647e-02,, Time: 9.63\n",
            "lambda_u: 4.834e+03\n",
            "lambda_ut: 2.479e+01\n",
            "lambda_r: 1.042e+00\n",
            "Compute NTK...\n",
            "It: 3900, Loss: 3.379e+01, Loss_res: 9.867e+00,  Loss_bcs: 4.169e-03, Loss_ut_ics: 1.159e-01,, Time: 10.26\n",
            "lambda_u: 4.936e+03\n",
            "lambda_ut: 2.535e+01\n",
            "lambda_r: 1.041e+00\n",
            "Compute NTK...\n",
            "It: 4000, Loss: 2.050e+01, Loss_res: 4.783e+00,  Loss_bcs: 3.147e-03, Loss_ut_ics: 1.404e-02,, Time: 10.07\n",
            "lambda_u: 4.817e+03\n",
            "lambda_ut: 2.519e+01\n",
            "lambda_r: 1.042e+00\n",
            "Compute NTK...\n",
            "It: 4100, Loss: 2.232e+01, Loss_res: 7.150e+00,  Loss_bcs: 2.713e-03, Loss_ut_ics: 2.746e-02,, Time: 9.63\n",
            "lambda_u: 5.222e+03\n",
            "lambda_ut: 2.614e+01\n",
            "lambda_r: 1.040e+00\n",
            "Compute NTK...\n",
            "It: 4200, Loss: 2.331e+01, Loss_res: 7.884e+00,  Loss_bcs: 2.456e-03, Loss_ut_ics: 1.783e-02,, Time: 9.59\n",
            "lambda_u: 5.951e+03\n",
            "lambda_ut: 2.975e+01\n",
            "lambda_r: 1.035e+00\n",
            "Compute NTK...\n",
            "It: 4300, Loss: 3.387e+01, Loss_res: 1.515e+01,  Loss_bcs: 2.599e-03, Loss_ut_ics: 1.159e-01,, Time: 9.60\n",
            "lambda_u: 5.722e+03\n",
            "lambda_ut: 2.837e+01\n",
            "lambda_r: 1.037e+00\n",
            "Compute NTK...\n",
            "It: 4400, Loss: 1.555e+01, Loss_res: 4.325e+00,  Loss_bcs: 1.830e-03, Loss_ut_ics: 1.192e-02,, Time: 9.63\n",
            "lambda_u: 5.858e+03\n",
            "lambda_ut: 2.922e+01\n",
            "lambda_r: 1.036e+00\n",
            "Compute NTK...\n",
            "It: 4500, Loss: 2.277e+01, Loss_res: 7.337e+00,  Loss_bcs: 2.000e-03, Loss_ut_ics: 8.437e-02,, Time: 9.62\n",
            "lambda_u: 6.345e+03\n",
            "lambda_ut: 2.948e+01\n",
            "lambda_r: 1.035e+00\n",
            "Compute NTK...\n",
            "It: 4600, Loss: 1.916e+01, Loss_res: 6.123e+00,  Loss_bcs: 1.656e-03, Loss_ut_ics: 9.655e-02,, Time: 9.57\n",
            "lambda_u: 6.012e+03\n",
            "lambda_ut: 2.967e+01\n",
            "lambda_r: 1.035e+00\n",
            "Compute NTK...\n",
            "It: 4700, Loss: 1.799e+01, Loss_res: 4.551e+00,  Loss_bcs: 1.812e-03, Loss_ut_ics: 7.614e-02,, Time: 9.57\n",
            "lambda_u: 6.101e+03\n",
            "lambda_ut: 2.919e+01\n",
            "lambda_r: 1.036e+00\n",
            "Compute NTK...\n",
            "It: 4800, Loss: 1.536e+01, Loss_res: 4.499e+00,  Loss_bcs: 1.709e-03, Loss_ut_ics: 3.545e-02,, Time: 9.55\n",
            "lambda_u: 5.676e+03\n",
            "lambda_ut: 2.805e+01\n",
            "lambda_r: 1.037e+00\n",
            "Compute NTK...\n",
            "It: 4900, Loss: 1.734e+01, Loss_res: 4.982e+00,  Loss_bcs: 1.735e-03, Loss_ut_ics: 5.315e-02,, Time: 9.61\n",
            "lambda_u: 6.109e+03\n",
            "lambda_ut: 2.973e+01\n",
            "lambda_r: 1.035e+00\n",
            "Compute NTK...\n",
            "It: 5000, Loss: 1.280e+01, Loss_res: 3.772e+00,  Loss_bcs: 1.354e-03, Loss_ut_ics: 2.875e-02,, Time: 9.55\n",
            "lambda_u: 5.943e+03\n",
            "lambda_ut: 2.934e+01\n",
            "lambda_r: 1.035e+00\n",
            "Compute NTK...\n",
            "It: 5100, Loss: 2.086e+01, Loss_res: 3.831e+00,  Loss_bcs: 2.005e-03, Loss_ut_ics: 1.584e-01,, Time: 9.58\n",
            "lambda_u: 6.135e+03\n",
            "lambda_ut: 2.900e+01\n",
            "lambda_r: 1.036e+00\n",
            "Compute NTK...\n",
            "It: 5200, Loss: 1.063e+01, Loss_res: 2.749e+00,  Loss_bcs: 1.229e-03, Loss_ut_ics: 1.366e-02,, Time: 9.84\n",
            "lambda_u: 6.027e+03\n",
            "lambda_ut: 2.698e+01\n",
            "lambda_r: 1.039e+00\n",
            "Compute NTK...\n",
            "It: 5300, Loss: 1.266e+01, Loss_res: 2.851e+00,  Loss_bcs: 1.534e-03, Loss_ut_ics: 9.933e-03,, Time: 9.59\n",
            "lambda_u: 6.128e+03\n",
            "lambda_ut: 3.255e+01\n",
            "lambda_r: 1.032e+00\n",
            "Compute NTK...\n",
            "It: 5400, Loss: 9.059e+00, Loss_res: 3.127e+00,  Loss_bcs: 9.224e-04, Loss_ut_ics: 6.528e-03,, Time: 9.54\n",
            "lambda_u: 6.106e+03\n",
            "lambda_ut: 2.854e+01\n",
            "lambda_r: 1.036e+00\n",
            "Compute NTK...\n",
            "It: 5500, Loss: 9.078e+00, Loss_res: 2.836e+00,  Loss_bcs: 8.591e-04, Loss_ut_ics: 1.792e-02,, Time: 9.62\n",
            "lambda_u: 6.504e+03\n",
            "lambda_ut: 3.127e+01\n",
            "lambda_r: 1.033e+00\n",
            "Compute NTK...\n",
            "It: 5600, Loss: 1.469e+01, Loss_res: 3.504e+00,  Loss_bcs: 1.005e-03, Loss_ut_ics: 1.719e-01,, Time: 9.57\n",
            "lambda_u: 6.016e+03\n",
            "lambda_ut: 2.920e+01\n",
            "lambda_r: 1.036e+00\n",
            "Compute NTK...\n",
            "It: 5700, Loss: 1.189e+01, Loss_res: 2.318e+00,  Loss_bcs: 8.922e-04, Loss_ut_ics: 1.255e-01,, Time: 9.63\n",
            "lambda_u: 6.145e+03\n",
            "lambda_ut: 3.200e+01\n",
            "lambda_r: 1.032e+00\n",
            "Compute NTK...\n",
            "It: 5800, Loss: 9.725e+00, Loss_res: 2.249e+00,  Loss_bcs: 9.504e-04, Loss_ut_ics: 5.205e-02,, Time: 9.64\n",
            "lambda_u: 6.119e+03\n",
            "lambda_ut: 3.043e+01\n",
            "lambda_r: 1.034e+00\n",
            "Compute NTK...\n",
            "It: 5900, Loss: 1.446e+01, Loss_res: 3.716e+00,  Loss_bcs: 8.831e-04, Loss_ut_ics: 1.757e-01,, Time: 9.63\n",
            "lambda_u: 6.107e+03\n",
            "lambda_ut: 2.974e+01\n",
            "lambda_r: 1.035e+00\n",
            "Compute NTK...\n",
            "It: 6000, Loss: 6.199e+00, Loss_res: 2.269e+00,  Loss_bcs: 6.578e-04, Loss_ut_ics: 3.505e-03,, Time: 9.71\n",
            "lambda_u: 5.698e+03\n",
            "lambda_ut: 2.596e+01\n",
            "lambda_r: 1.040e+00\n",
            "Compute NTK...\n",
            "It: 6100, Loss: 7.534e+00, Loss_res: 2.349e+00,  Loss_bcs: 7.068e-04, Loss_ut_ics: 2.081e-02,, Time: 9.60\n",
            "lambda_u: 6.275e+03\n",
            "lambda_ut: 3.239e+01\n",
            "lambda_r: 1.032e+00\n",
            "Compute NTK...\n",
            "It: 6200, Loss: 7.202e+00, Loss_res: 2.504e+00,  Loss_bcs: 6.670e-04, Loss_ut_ics: 1.300e-02,, Time: 9.71\n",
            "lambda_u: 6.308e+03\n",
            "lambda_ut: 3.141e+01\n",
            "lambda_r: 1.033e+00\n",
            "Compute NTK...\n",
            "It: 6300, Loss: 6.237e+00, Loss_res: 1.974e+00,  Loss_bcs: 6.694e-04, Loss_ut_ics: 1.113e-02,, Time: 9.75\n",
            "lambda_u: 5.767e+03\n",
            "lambda_ut: 3.007e+01\n",
            "lambda_r: 1.035e+00\n",
            "Compute NTK...\n",
            "It: 6400, Loss: 8.147e+00, Loss_res: 2.598e+00,  Loss_bcs: 7.171e-04, Loss_ut_ics: 3.223e-02,, Time: 9.56\n",
            "lambda_u: 6.233e+03\n",
            "lambda_ut: 3.077e+01\n",
            "lambda_r: 1.034e+00\n",
            "Compute NTK...\n",
            "It: 6500, Loss: 6.550e+00, Loss_res: 2.032e+00,  Loss_bcs: 6.410e-04, Loss_ut_ics: 7.142e-03,, Time: 9.99\n",
            "lambda_u: 6.582e+03\n",
            "lambda_ut: 3.279e+01\n",
            "lambda_r: 1.032e+00\n",
            "Compute NTK...\n",
            "It: 6600, Loss: 8.026e+00, Loss_res: 1.857e+00,  Loss_bcs: 6.341e-04, Loss_ut_ics: 6.224e-02,, Time: 9.54\n",
            "lambda_u: 6.523e+03\n",
            "lambda_ut: 3.169e+01\n",
            "lambda_r: 1.033e+00\n",
            "Compute NTK...\n",
            "It: 6700, Loss: 4.673e+00, Loss_res: 1.622e+00,  Loss_bcs: 4.207e-04, Loss_ut_ics: 1.339e-02,, Time: 9.50\n",
            "lambda_u: 6.164e+03\n",
            "lambda_ut: 3.000e+01\n",
            "lambda_r: 1.035e+00\n",
            "Compute NTK...\n",
            "It: 6800, Loss: 5.024e+00, Loss_res: 1.558e+00,  Loss_bcs: 5.266e-04, Loss_ut_ics: 6.226e-03,, Time: 9.96\n",
            "lambda_u: 6.117e+03\n",
            "lambda_ut: 3.092e+01\n",
            "lambda_r: 1.034e+00\n",
            "Compute NTK...\n",
            "It: 6900, Loss: 7.866e+00, Loss_res: 2.627e+00,  Loss_bcs: 6.498e-04, Loss_ut_ics: 1.380e-02,, Time: 9.65\n",
            "lambda_u: 7.211e+03\n",
            "lambda_ut: 3.431e+01\n",
            "lambda_r: 1.030e+00\n",
            "Compute NTK...\n",
            "It: 7000, Loss: 6.422e+00, Loss_res: 1.579e+00,  Loss_bcs: 5.849e-04, Loss_ut_ics: 2.792e-02,, Time: 9.66\n",
            "lambda_u: 6.606e+03\n",
            "lambda_ut: 3.334e+01\n",
            "lambda_r: 1.031e+00\n",
            "Compute NTK...\n",
            "It: 7100, Loss: 4.005e+00, Loss_res: 1.285e+00,  Loss_bcs: 3.876e-04, Loss_ut_ics: 7.844e-03,, Time: 9.61\n",
            "lambda_u: 6.265e+03\n",
            "lambda_ut: 3.185e+01\n",
            "lambda_r: 1.033e+00\n",
            "Compute NTK...\n",
            "It: 7200, Loss: 3.555e+00, Loss_res: 1.329e+00,  Loss_bcs: 3.626e-04, Loss_ut_ics: 2.396e-03,, Time: 9.64\n",
            "lambda_u: 5.818e+03\n",
            "lambda_ut: 2.789e+01\n",
            "lambda_r: 1.037e+00\n",
            "Compute NTK...\n",
            "It: 7300, Loss: 5.165e+00, Loss_res: 2.004e+00,  Loss_bcs: 4.113e-04, Loss_ut_ics: 1.979e-02,, Time: 9.56\n",
            "lambda_u: 6.077e+03\n",
            "lambda_ut: 2.990e+01\n",
            "lambda_r: 1.035e+00\n",
            "Compute NTK...\n",
            "It: 7400, Loss: 4.754e+00, Loss_res: 1.457e+00,  Loss_bcs: 4.945e-04, Loss_ut_ics: 3.544e-03,, Time: 12.09\n",
            "lambda_u: 6.341e+03\n",
            "lambda_ut: 3.235e+01\n",
            "lambda_r: 1.032e+00\n",
            "Compute NTK...\n",
            "It: 7500, Loss: 4.495e+00, Loss_res: 1.569e+00,  Loss_bcs: 4.457e-04, Loss_ut_ics: 4.066e-03,, Time: 11.09\n",
            "lambda_u: 6.166e+03\n",
            "lambda_ut: 3.073e+01\n",
            "lambda_r: 1.034e+00\n",
            "Compute NTK...\n",
            "It: 7600, Loss: 5.781e+00, Loss_res: 2.585e+00,  Loss_bcs: 3.922e-04, Loss_ut_ics: 1.207e-02,, Time: 10.11\n",
            "lambda_u: 6.971e+03\n",
            "lambda_ut: 3.119e+01\n",
            "lambda_r: 1.033e+00\n",
            "Compute NTK...\n",
            "It: 7700, Loss: 3.711e+00, Loss_res: 9.396e-01,  Loss_bcs: 3.605e-04, Loss_ut_ics: 9.113e-03,, Time: 10.35\n",
            "lambda_u: 6.813e+03\n",
            "lambda_ut: 3.118e+01\n",
            "lambda_r: 1.033e+00\n",
            "Compute NTK...\n",
            "It: 7800, Loss: 4.353e+00, Loss_res: 1.361e+00,  Loss_bcs: 4.197e-04, Loss_ut_ics: 7.330e-03,, Time: 9.93\n",
            "lambda_u: 6.463e+03\n",
            "lambda_ut: 3.207e+01\n",
            "lambda_r: 1.032e+00\n",
            "Compute NTK...\n",
            "It: 7900, Loss: 3.247e+00, Loss_res: 1.271e+00,  Loss_bcs: 2.984e-04, Loss_ut_ics: 2.176e-03,, Time: 10.25\n",
            "lambda_u: 6.256e+03\n",
            "lambda_ut: 2.945e+01\n",
            "lambda_r: 1.035e+00\n",
            "Compute NTK...\n",
            "It: 8000, Loss: 3.126e+00, Loss_res: 7.653e-01,  Loss_bcs: 3.776e-04, Loss_ut_ics: 4.723e-03,, Time: 9.61\n",
            "lambda_u: 5.819e+03\n",
            "lambda_ut: 2.875e+01\n",
            "lambda_r: 1.036e+00\n",
            "Compute NTK...\n",
            "It: 8100, Loss: 3.358e+00, Loss_res: 8.586e-01,  Loss_bcs: 2.888e-04, Loss_ut_ics: 2.400e-02,, Time: 9.57\n",
            "lambda_u: 6.092e+03\n",
            "lambda_ut: 2.958e+01\n",
            "lambda_r: 1.035e+00\n",
            "Compute NTK...\n",
            "It: 8200, Loss: 3.507e+00, Loss_res: 1.004e+00,  Loss_bcs: 2.728e-04, Loss_ut_ics: 2.169e-02,, Time: 9.61\n",
            "lambda_u: 6.462e+03\n",
            "lambda_ut: 3.261e+01\n",
            "lambda_r: 1.032e+00\n",
            "Compute NTK...\n",
            "It: 8300, Loss: 2.400e+00, Loss_res: 8.062e-01,  Loss_bcs: 2.087e-04, Loss_ut_ics: 3.761e-03,, Time: 9.65\n",
            "lambda_u: 6.914e+03\n",
            "lambda_ut: 3.344e+01\n",
            "lambda_r: 1.031e+00\n",
            "Compute NTK...\n",
            "It: 8400, Loss: 4.724e+00, Loss_res: 7.811e-01,  Loss_bcs: 5.425e-04, Loss_ut_ics: 1.003e-02,, Time: 9.68\n",
            "lambda_u: 6.609e+03\n",
            "lambda_ut: 3.317e+01\n",
            "lambda_r: 1.031e+00\n",
            "Compute NTK...\n",
            "It: 8500, Loss: 2.611e+00, Loss_res: 6.801e-01,  Loss_bcs: 2.389e-04, Loss_ut_ics: 9.754e-03,, Time: 9.67\n",
            "lambda_u: 6.674e+03\n",
            "lambda_ut: 3.225e+01\n",
            "lambda_r: 1.032e+00\n",
            "Compute NTK...\n",
            "It: 8600, Loss: 1.949e+00, Loss_res: 5.254e-01,  Loss_bcs: 1.966e-04, Loss_ut_ics: 5.124e-03,, Time: 9.67\n",
            "lambda_u: 6.360e+03\n",
            "lambda_ut: 3.031e+01\n",
            "lambda_r: 1.034e+00\n",
            "Compute NTK...\n",
            "It: 8700, Loss: 2.334e+00, Loss_res: 7.778e-01,  Loss_bcs: 1.944e-04, Loss_ut_ics: 6.854e-03,, Time: 9.59\n",
            "lambda_u: 6.691e+03\n",
            "lambda_ut: 3.374e+01\n",
            "lambda_r: 1.031e+00\n",
            "Compute NTK...\n",
            "It: 8800, Loss: 2.132e+00, Loss_res: 7.928e-01,  Loss_bcs: 1.762e-04, Loss_ut_ics: 5.700e-03,, Time: 9.60\n",
            "lambda_u: 6.466e+03\n",
            "lambda_ut: 3.018e+01\n",
            "lambda_r: 1.034e+00\n",
            "Compute NTK...\n",
            "It: 8900, Loss: 1.983e+00, Loss_res: 6.554e-01,  Loss_bcs: 1.753e-04, Loss_ut_ics: 4.712e-03,, Time: 9.66\n",
            "lambda_u: 6.593e+03\n",
            "lambda_ut: 3.187e+01\n",
            "lambda_r: 1.033e+00\n",
            "Compute NTK...\n",
            "It: 9000, Loss: 2.304e+00, Loss_res: 8.520e-01,  Loss_bcs: 1.809e-04, Loss_ut_ics: 8.760e-03,, Time: 9.65\n",
            "lambda_u: 6.254e+03\n",
            "lambda_ut: 3.357e+01\n",
            "lambda_r: 1.031e+00\n",
            "Compute NTK...\n",
            "It: 9100, Loss: 1.671e+00, Loss_res: 5.801e-01,  Loss_bcs: 1.699e-04, Loss_ut_ics: 1.558e-03,, Time: 9.71\n",
            "lambda_u: 6.030e+03\n",
            "lambda_ut: 2.930e+01\n",
            "lambda_r: 1.036e+00\n",
            "Compute NTK...\n",
            "It: 9200, Loss: 2.463e+00, Loss_res: 8.053e-01,  Loss_bcs: 1.833e-04, Loss_ut_ics: 1.296e-02,, Time: 9.59\n",
            "lambda_u: 6.592e+03\n",
            "lambda_ut: 3.271e+01\n",
            "lambda_r: 1.032e+00\n",
            "Compute NTK...\n",
            "It: 9300, Loss: 1.642e+00, Loss_res: 5.816e-01,  Loss_bcs: 1.490e-04, Loss_ut_ics: 3.439e-03,, Time: 9.69\n",
            "lambda_u: 6.275e+03\n",
            "lambda_ut: 3.085e+01\n",
            "lambda_r: 1.034e+00\n",
            "Compute NTK...\n",
            "It: 9400, Loss: 2.594e+00, Loss_res: 7.092e-01,  Loss_bcs: 1.372e-04, Loss_ut_ics: 2.697e-02,, Time: 9.75\n",
            "lambda_u: 6.866e+03\n",
            "lambda_ut: 3.415e+01\n",
            "lambda_r: 1.030e+00\n",
            "Compute NTK...\n",
            "It: 9500, Loss: 2.914e+00, Loss_res: 6.827e-01,  Loss_bcs: 3.088e-04, Loss_ut_ics: 4.739e-03,, Time: 9.70\n",
            "lambda_u: 6.679e+03\n",
            "lambda_ut: 3.069e+01\n",
            "lambda_r: 1.034e+00\n",
            "Compute NTK...\n",
            "It: 9600, Loss: 2.152e+00, Loss_res: 6.818e-01,  Loss_bcs: 1.846e-04, Loss_ut_ics: 7.352e-03,, Time: 9.74\n",
            "lambda_u: 6.591e+03\n",
            "lambda_ut: 3.149e+01\n",
            "lambda_r: 1.033e+00\n",
            "Compute NTK...\n",
            "It: 9700, Loss: 1.692e+00, Loss_res: 6.519e-01,  Loss_bcs: 1.339e-04, Loss_ut_ics: 5.202e-03,, Time: 9.66\n",
            "lambda_u: 6.386e+03\n",
            "lambda_ut: 3.135e+01\n",
            "lambda_r: 1.033e+00\n",
            "Compute NTK...\n",
            "It: 9800, Loss: 4.664e+00, Loss_res: 5.740e-01,  Loss_bcs: 2.152e-04, Loss_ut_ics: 7.953e-02,, Time: 9.62\n",
            "lambda_u: 6.843e+03\n",
            "lambda_ut: 3.268e+01\n",
            "lambda_r: 1.032e+00\n",
            "Compute NTK...\n",
            "It: 9900, Loss: 1.782e+00, Loss_res: 5.092e-01,  Loss_bcs: 1.196e-04, Loss_ut_ics: 1.564e-02,, Time: 9.54\n",
            "lambda_u: 6.279e+03\n",
            "lambda_ut: 3.230e+01\n",
            "lambda_r: 1.032e+00\n",
            "Compute NTK...\n",
            "It: 10000, Loss: 2.000e+00, Loss_res: 5.350e-01,  Loss_bcs: 2.027e-04, Loss_ut_ics: 5.718e-03,, Time: 9.59\n",
            "lambda_u: 6.277e+03\n",
            "lambda_ut: 3.050e+01\n",
            "lambda_r: 1.034e+00\n",
            "Compute NTK...\n",
            "It: 10100, Loss: 1.966e+00, Loss_res: 4.868e-01,  Loss_bcs: 1.599e-04, Loss_ut_ics: 1.014e-02,, Time: 9.68\n",
            "lambda_u: 6.896e+03\n",
            "lambda_ut: 3.571e+01\n",
            "lambda_r: 1.029e+00\n",
            "Compute NTK...\n",
            "It: 10200, Loss: 4.710e+00, Loss_res: 2.127e+00,  Loss_bcs: 2.725e-04, Loss_ut_ics: 1.956e-02,, Time: 9.66\n",
            "lambda_u: 6.743e+03\n",
            "lambda_ut: 3.489e+01\n",
            "lambda_r: 1.030e+00\n",
            "Compute NTK...\n",
            "It: 10300, Loss: 2.265e+00, Loss_res: 8.201e-01,  Loss_bcs: 1.883e-04, Loss_ut_ics: 4.823e-03,, Time: 9.66\n",
            "lambda_u: 6.685e+03\n",
            "lambda_ut: 3.333e+01\n",
            "lambda_r: 1.031e+00\n",
            "Compute NTK...\n",
            "It: 10400, Loss: 1.120e+00, Loss_res: 3.476e-01,  Loss_bcs: 8.010e-05, Loss_ut_ics: 8.022e-03,, Time: 9.71\n",
            "lambda_u: 6.268e+03\n",
            "lambda_ut: 3.226e+01\n",
            "lambda_r: 1.032e+00\n",
            "Compute NTK...\n",
            "It: 10500, Loss: 8.015e-01, Loss_res: 2.271e-01,  Loss_bcs: 8.063e-05, Loss_ut_ics: 1.654e-03,, Time: 9.62\n",
            "lambda_u: 6.370e+03\n",
            "lambda_ut: 3.229e+01\n",
            "lambda_r: 1.032e+00\n",
            "Compute NTK...\n",
            "It: 10600, Loss: 1.607e+00, Loss_res: 4.959e-01,  Loss_bcs: 1.184e-04, Loss_ut_ics: 9.224e-03,, Time: 9.62\n",
            "lambda_u: 6.682e+03\n",
            "lambda_ut: 3.299e+01\n",
            "lambda_r: 1.031e+00\n",
            "Compute NTK...\n",
            "It: 10700, Loss: 1.013e+00, Loss_res: 3.835e-01,  Loss_bcs: 8.962e-05, Loss_ut_ics: 2.138e-03,, Time: 9.71\n",
            "lambda_u: 6.137e+03\n",
            "lambda_ut: 3.132e+01\n",
            "lambda_r: 1.033e+00\n",
            "Compute NTK...\n",
            "It: 10800, Loss: 3.489e+00, Loss_res: 7.073e-01,  Loss_bcs: 2.024e-04, Loss_ut_ics: 4.072e-02,, Time: 9.95\n",
            "lambda_u: 6.810e+03\n",
            "lambda_ut: 3.393e+01\n",
            "lambda_r: 1.031e+00\n",
            "Compute NTK...\n",
            "It: 10900, Loss: 9.037e-01, Loss_res: 3.233e-01,  Loss_bcs: 8.555e-05, Loss_ut_ics: 1.269e-03,, Time: 9.60\n",
            "lambda_u: 6.182e+03\n",
            "lambda_ut: 3.245e+01\n",
            "lambda_r: 1.032e+00\n",
            "Compute NTK...\n",
            "It: 11000, Loss: 1.422e+00, Loss_res: 3.167e-01,  Loss_bcs: 1.293e-04, Loss_ut_ics: 5.337e-03,, Time: 10.14\n",
            "lambda_u: 7.054e+03\n",
            "lambda_ut: 3.437e+01\n",
            "lambda_r: 1.030e+00\n",
            "Compute NTK...\n",
            "It: 11100, Loss: 1.855e+00, Loss_res: 7.187e-01,  Loss_bcs: 1.425e-04, Loss_ut_ics: 5.605e-03,, Time: 10.08\n",
            "lambda_u: 6.519e+03\n",
            "lambda_ut: 3.294e+01\n",
            "lambda_r: 1.031e+00\n",
            "Compute NTK...\n"
          ]
        }
      ],
      "source": [
        "# Define PINN model\n",
        "a = 0.5\n",
        "c = 2\n",
        "\n",
        "kernel_size = 300\n",
        "\n",
        "# Domain boundaries\n",
        "ics_coords = np.array([[0.0, 0.0],  [0.0, 1.0]])\n",
        "bc1_coords = np.array([[0.0, 0.0],  [1.0, 0.0]])\n",
        "bc2_coords = np.array([[0.0, 1.0],  [1.0, 1.0]])\n",
        "dom_coords = np.array([[0.0, 0.0],  [1.0, 1.0]])\n",
        "\n",
        "# Create initial conditions samplers\n",
        "ics_sampler = Sampler(2, ics_coords, lambda x: u(x, a, c), name='Initial Condition 1')\n",
        "\n",
        "# Create boundary conditions samplers\n",
        "bc1 = Sampler(2, bc1_coords, lambda x: u(x, a, c), name='Dirichlet BC1')\n",
        "bc2 = Sampler(2, bc2_coords, lambda x: u(x, a, c), name='Dirichlet BC2')\n",
        "bcs_sampler = [bc1, bc2]\n",
        "\n",
        "# Create residual sampler\n",
        "res_sampler = Sampler(2, dom_coords, lambda x: r(x, a, c), name='Forcing')\n",
        "\n",
        "\n",
        "\n",
        "nIter =40000\n",
        "bcbatch_size = 350\n",
        "ubatch_size = 5000\n",
        "mbbatch_size = 300\n",
        "\n",
        "\n",
        "\n",
        "# Define model\n",
        "mode = 'M3'\n",
        "layers = [2, 500, 500, 500, 1]\n",
        "\n",
        "\n",
        "nn = 200\n",
        "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
        "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
        "t, x = np.meshgrid(t, x)\n",
        "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
        "\n",
        "u_star = u(X_star, a,c)\n",
        "r_star = r(X_star, a, c)\n",
        "\n",
        "iterations = 1\n",
        "methods = [  \"mini_batch\"]\n",
        "\n",
        "result_dict =  dict((mtd, []) for mtd in methods)\n",
        "\n",
        "for mtd in methods:\n",
        "    print(\"Method: \", mtd)\n",
        "    time_list = []\n",
        "    error_u_list = []\n",
        "    \n",
        "    for index in range(iterations):\n",
        "\n",
        "        print(\"Epoch: \", str(index+1))\n",
        "\n",
        "        # Create residual sampler\n",
        "\n",
        "        [elapsed, error_u] = test_method(mtd , layers,  ics_sampler, bcs_sampler, res_sampler, c ,kernel_size , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size )\n",
        "\n",
        "\n",
        "        print('elapsed: {:.2e}'.format(elapsed))\n",
        "        print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "\n",
        "        time_list.append(elapsed)\n",
        "        error_u_list.append(error_u)\n",
        "\n",
        "    print(\"\\n\\nMethod: \", mtd)\n",
        "    print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
        "    print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
        "    # print(\"average of error_r_list:\" , sum(error_r_list) / len(error_r_list) )\n",
        "\n",
        "    result_dict[mtd] = [time_list ,error_u_list]\n",
        "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
        "\n",
        "    scipy.io.savemat(\"./1DWave_database/mini_batch_1Dwave_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_\"+str(iterations)+\".mat\" , result_dict)\n",
        "\n",
        "###############################################################################################################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZtWEM9-brXF",
        "outputId": "371feba5-fed5-41cb-9e3e-5c8497c4fbe6"
      },
      "outputs": [],
      "source": [
        "\n",
        "import scipy.io\n",
        "\n",
        "mode = 'M4'\n",
        "mbbatch_size = 128\n",
        "ubatch_size = 5000\n",
        "bcbatch_size = 500\n",
        "iterations = 40000\n",
        "\n",
        "time_list = []\n",
        "error_u_list = []\n",
        "error_v_list = []\n",
        "error_p_list = []\n",
        "    \n",
        "methods = [\"mini_batch\" , \"full_batch\"]\n",
        "result_dict =  dict((mtd, []) for mtd in methods)\n",
        "\n",
        "##Mini Batch\n",
        "time_list = [3124.91]\n",
        "error_u_list = [ 0.0041]\n",
        "\n",
        "\n",
        "result_dict[\"mini_batch\"] = [time_list ,error_u_list]\n",
        "\n",
        "print(\"\\n\\nMethod: \", mtd)\n",
        "print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
        "print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
        "\n",
        "##Full Batch\n",
        "time_list = []\n",
        "error_u_list = [ ]\n",
        "error_v_list = []\n",
        "error_p_list = []\n",
        "\n",
        "result_dict[\"full_batch\"] = [time_list ,error_u_list ,error_v_list ,  error_p_list]\n",
        "\n",
        "print(\"\\n\\nMethod: \", mtd)\n",
        "print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
        "print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
        "print(\"average of error_v_list:\" , sum(error_v_list) / len(error_v_list) )\n",
        "print(\"average of error_p_list:\" , sum(error_p_list) / len(error_p_list) )\n",
        "\n",
        "\n",
        "scipy.io.savemat(\"./dataset/NS_model_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_\"+str(bcbatch_size)+\"_\"+str(iterations)+\".mat\" , result_dict)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vF1hwPUobyPE"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "itertaions = 80001\n",
        "log_NTK = True # Compute and store NTK matrix during training\n",
        "update_lam = True # Compute and update the loss weights using the NTK \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiyikOwBjRoZ"
      },
      "source": [
        "**Training Loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "Fw807UNzhu5z",
        "outputId": "f4551313-ffbc-49b5-8fd3-1296fc1641fe"
      },
      "outputs": [],
      "source": [
        "loss_res = model.loss_res_log\n",
        "loss_bcs = model.loss_bcs_log\n",
        "loss_u_t_ics = model.loss_ut_ics_log\n",
        "\n",
        "fig = plt.figure(figsize=(6, 5))\n",
        "plt.plot(loss_res, label='$\\mathcal{L}_{r}$')\n",
        "plt.plot(loss_bcs, label='$\\mathcal{L}_{u}$')\n",
        "plt.plot(loss_u_t_ics, label='$\\mathcal{L}_{u_t}$')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFLIBq5xjZ3v"
      },
      "source": [
        "**Model Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To0PDN17cc0v",
        "outputId": "1f47f288-322a-46b5-f173-45485191a68d"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Predictions\n",
        "u_pred = model.predict_u(X_star)\n",
        "r_pred = model.predict_r(X_star)\n",
        "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "\n",
        "print('Relative L2 error_u: %e' % (error_u))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "K428lOuXhdc8",
        "outputId": "015f591b-d8a4-4e47-8020-84fcf219d7ca"
      },
      "outputs": [],
      "source": [
        "U_star = griddata(X_star, u_star.flatten(), (t, x), method='cubic')\n",
        "r_star = griddata(X_star, r_star.flatten(), (t, x), method='cubic')\n",
        "U_pred = griddata(X_star, u_pred.flatten(), (t, x), method='cubic')\n",
        "R_pred = griddata(X_star, r_pred.flatten(), (t, x), method='cubic')\n",
        "\n",
        "\n",
        "plt.figure(figsize=(18, 9))\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.pcolor(t, x, U_star, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "plt.title('Exact u(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.pcolor(t, x, U_pred, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Predicted u(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.pcolor(t, x, np.abs(U_star - U_pred), cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Absolute error')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.pcolor(t, x, r_star, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Exact r(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.pcolor(t, x, R_pred, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Predicted r(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 6)\n",
        "plt.pcolor(t, x, np.abs(r_star - R_pred), cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Absolute error')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EYdfKGLj6h0"
      },
      "source": [
        "**NTK Eigenvalues**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3dByeQjhBYj"
      },
      "outputs": [],
      "source": [
        "# Create empty lists for storing the eigenvalues of NTK\n",
        "lam_K_u_log = []\n",
        "lam_K_ut_log = []\n",
        "lam_K_r_log = []\n",
        "\n",
        "# Restore the NTK\n",
        "K_u_list = model.K_u_log\n",
        "K_ut_list = model.K_ut_log\n",
        "K_r_list = model.K_r_log\n",
        "\n",
        "K_list = []\n",
        "    \n",
        "for k in range(len(K_u_list)):\n",
        "    K_u = K_u_list[k]\n",
        "    K_ut = K_ut_list[k]\n",
        "    K_r = K_r_list[k]\n",
        "    \n",
        "    # Compute eigenvalues\n",
        "    lam_K_u, _ = np.linalg.eig(K_u)\n",
        "    lam_K_ut, _ = np.linalg.eig(K_ut)\n",
        "    lam_K_r, _ = np.linalg.eig(K_r)\n",
        "    # Sort in descresing order\n",
        "    lam_K_u = np.sort(np.real(lam_K_u))[::-1]\n",
        "    lam_K_ut = np.sort(np.real(lam_K_ut))[::-1]\n",
        "    lam_K_r = np.sort(np.real(lam_K_r))[::-1]\n",
        "    \n",
        "    # Store eigenvalues\n",
        "    lam_K_u_log.append(lam_K_u)\n",
        "    lam_K_ut_log.append(lam_K_ut)\n",
        "    lam_K_r_log.append(lam_K_r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "vSn3Q_1IhisN",
        "outputId": "886908b3-c316-48d6-933f-81b1180ff954"
      },
      "outputs": [],
      "source": [
        "#  Eigenvalues of NTK\n",
        "fig = plt.figure(figsize=(18, 5))\n",
        "plt.subplot(1,3,1)\n",
        "\n",
        "plt.plot(lam_K_u_log[0], label = '$n=0$')\n",
        "plt.plot(lam_K_u_log[1], '--', label = '$n=10,000$')\n",
        "plt.plot(lam_K_u_log[4], '--', label = '$n=40,000$')\n",
        "plt.plot(lam_K_u_log[-1], '--', label = '$n=80,000$')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.title(r'Eigenvalues of ${K}_u$')\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.plot(lam_K_ut_log[0], label = '$n=0$')\n",
        "plt.plot(lam_K_ut_log[1], '--',label = '$n=10,000$')\n",
        "plt.plot(lam_K_ut_log[4], '--', label = '$n=40,000$')\n",
        "plt.plot(lam_K_ut_log[-1], '--', label = '$n=80,000$')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.title(r'Eigenvalues of ${K}_{u_t}$')\n",
        "\n",
        "ax =plt.subplot(1,3,3)\n",
        "plt.plot(lam_K_r_log[0], label = '$n=0$')\n",
        "plt.plot(lam_K_r_log[1], '--', label = '$n=10,000$')\n",
        "plt.plot(lam_K_r_log[4], '--', label = '$n=40,000$')\n",
        "plt.plot(lam_K_r_log[-1], '--', label = '$n=80,000$')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.title(r'Eigenvalues of ${K}_{r}$')\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "fig.legend(handles, labels, loc=\"upper left\", bbox_to_anchor=(0.35, -0.02),\n",
        "            borderaxespad=0, bbox_transform=fig.transFigure, ncol=4)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbUn_fcowojl"
      },
      "source": [
        "**Evolution of NTK Weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYbzkhfMjJ8k"
      },
      "outputs": [],
      "source": [
        "if update_lam == True:\n",
        "\n",
        "  lam_u_log = model.lam_u_log\n",
        "  lam_ut_log = model.lam_ut_log\n",
        "  lam_r_log = model.lam_r_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "xzFzPCA2w1ML",
        "outputId": "71452cf9-3ebb-4aeb-9708-c7664b88e65d"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6, 5))\n",
        "plt.plot(lam_u_log, label='$\\lambda_u$')\n",
        "plt.plot(lam_ut_log, label='$\\lambda_{u_t}$')\n",
        "plt.plot(lam_r_log, label='$\\lambda_{r}$')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('$\\lambda$')\n",
        "plt.yscale('log')\n",
        "plt.legend( )\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mimIv2Z5xlip"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PINNsNTK_Wave.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
