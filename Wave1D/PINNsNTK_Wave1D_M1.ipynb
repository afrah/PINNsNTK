{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7WkCgnRiYQSY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from Compute_Jacobian import jacobian # Please download 'Compute_Jacobian.py' in the repository \n",
        "import numpy as np\n",
        "import timeit\n",
        "from scipy.interpolate import griddata\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
        "import timeit\n",
        "\n",
        "import sys\n",
        "\n",
        "import scipy\n",
        "import scipy.io\n",
        "import time\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-y7cHTcJfBTR"
      },
      "outputs": [],
      "source": [
        "class Sampler:\n",
        "    # Initialize the class\n",
        "    def __init__(self, dim, coords, func, name = None):\n",
        "        self.dim = dim\n",
        "        self.coords = coords\n",
        "        self.func = func\n",
        "        self.name = name\n",
        "    def sample(self, N):\n",
        "        x = self.coords[0:1,:] + (self.coords[1:2,:]-self.coords[0:1,:])*np.random.rand(N, self.dim)\n",
        "        y = self.func(x)\n",
        "        return x, y\n",
        "\n",
        "# Define the exact solution and its derivatives\n",
        "def u(x, a, c):\n",
        "    \"\"\"\n",
        "    :param x: x = (t, x)\n",
        "    \"\"\"\n",
        "    t = x[:,0:1]\n",
        "    x = x[:,1:2]\n",
        "    return np.sin(np.pi * x) * np.cos(c * np.pi * t) + a * np.sin(2 * c * np.pi* x) * np.cos(4 * c  * np.pi * t)\n",
        "\n",
        "def u_t(x,a, c):\n",
        "    t = x[:,0:1]\n",
        "    x = x[:,1:2]\n",
        "    u_t = -  c * np.pi * np.sin(np.pi * x) * np.sin(c * np.pi * t) -  a * 4 * c * np.pi * np.sin(2 * c * np.pi* x) * np.sin(4 * c * np.pi * t)\n",
        "    return u_t\n",
        "\n",
        "def u_tt(x, a, c):\n",
        "    t = x[:,0:1]\n",
        "    x = x[:,1:2]\n",
        "    u_tt = -(c * np.pi)**2 * np.sin( np.pi * x) * np.cos(c * np.pi * t) - a * (4 * c * np.pi)**2 *  np.sin(2 * c * np.pi* x) * np.cos(4 * c * np.pi * t)\n",
        "    return u_tt\n",
        "\n",
        "def u_xx(x, a, c):\n",
        "    t = x[:,0:1]\n",
        "    x = x[:,1:2]\n",
        "    u_xx = - np.pi**2 * np.sin( np.pi * x) * np.cos(c * np.pi * t) -  a * (2 * c * np.pi)** 2 * np.sin(2 * c * np.pi* x) * np.cos(4 * c * np.pi * t)\n",
        "    return  u_xx\n",
        "\n",
        "\n",
        "def r(x, a, c):\n",
        "    return u_tt(x, a, c) - c**2 * u_xx(x, a, c)\n",
        "\n",
        "def operator(u, t, x, c, sigma_t=1.0, sigma_x=1.0):\n",
        "    u_t = tf.gradients(u, t)[0] / sigma_t\n",
        "    u_x = tf.gradients(u, x)[0] / sigma_x\n",
        "    u_tt = tf.gradients(u_t, t)[0] / sigma_t\n",
        "    u_xx = tf.gradients(u_x, x)[0] / sigma_x\n",
        "    residual = u_tt - c**2 * u_xx\n",
        "    return residual\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SDqDWN3nfSAg"
      },
      "outputs": [],
      "source": [
        "class PINN:\n",
        "    # Initialize the class\n",
        "    def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, c, kernel_size , sess):\n",
        "        # Normalization \n",
        "        X, _ = res_sampler.sample(np.int32(1e5))\n",
        "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
        "        self.mu_t, self.sigma_t = self.mu_X[0], self.sigma_X[0]\n",
        "        self.mu_x, self.sigma_x = self.mu_X[1], self.sigma_X[1]\n",
        "\n",
        "        # Samplers\n",
        "        self.operator = operator\n",
        "        self.ics_sampler = ics_sampler\n",
        "        self.bcs_sampler = bcs_sampler\n",
        "        self.res_sampler = res_sampler\n",
        "\n",
        "        self.sess = sess\n",
        "        # Initialize network weights and biases\n",
        "        self.layers = layers\n",
        "        self.weights, self.biases = self.initialize_NN(layers)\n",
        "        \n",
        "        # weights\n",
        "        self.lam_u_val = np.array(1.0)\n",
        "        self.lam_ut_val = np.array(1.0)\n",
        "        self.lam_r_val = np.array(1.0)\n",
        "      \n",
        "        # Wave constant\n",
        "        self.c = tf.constant(c, dtype=tf.float32)\n",
        "        \n",
        "        self.kernel_size = kernel_size # Size of the NTK matrix\n",
        "\n",
        "        # Define Tensorflow session\n",
        "        self.sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
        "\n",
        "        # Define placeholders and computational graph\n",
        "        self.t_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.x_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.t_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.x_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.u_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.t_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.x_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.t_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.x_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.t_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.x_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        \n",
        "        self.lam_u_tf = tf.placeholder(tf.float32, shape=self.lam_u_val.shape)\n",
        "        self.lam_ut_tf = tf.placeholder(tf.float32, shape=self.lam_u_val.shape)\n",
        "        self.lam_r_tf = tf.placeholder(tf.float32, shape=self.lam_u_val.shape)\n",
        "        \n",
        "\n",
        "        # Define placeholders for NTK computation\n",
        "        D1 = self.kernel_size    # boundary\n",
        "        D2 = self.kernel_size    # ut   \n",
        "        D3 = self.kernel_size    # residual  D1 = D3 = 3D2\n",
        "\n",
        "        self.t_u_ntk_tf = tf.placeholder(tf.float32, shape=(D1, 1))\n",
        "        self.x_u_ntk_tf = tf.placeholder(tf.float32, shape=(D1, 1))\n",
        "        \n",
        "        self.t_ut_ntk_tf = tf.placeholder(tf.float32, shape=(D2, 1))\n",
        "        self.x_ut_ntk_tf = tf.placeholder(tf.float32, shape=(D2, 1))\n",
        "        \n",
        "        self.t_r_ntk_tf = tf.placeholder(tf.float32, shape=(D3, 1))\n",
        "        self.x_r_ntk_tf = tf.placeholder(tf.float32, shape=(D3, 1))\n",
        "\n",
        "        # Evaluate predictions\n",
        "        self.u_ics_pred = self.net_u(self.t_ics_tf, self.x_ics_tf)\n",
        "        self.u_t_ics_pred = self.net_u_t(self.t_ics_tf, self.x_ics_tf)\n",
        "        self.u_bc1_pred = self.net_u(self.t_bc1_tf, self.x_bc1_tf)\n",
        "        self.u_bc2_pred = self.net_u(self.t_bc2_tf, self.x_bc2_tf)\n",
        "\n",
        "        self.u_pred = self.net_u(self.t_u_tf, self.x_u_tf)\n",
        "        self.r_pred = self.net_r(self.t_r_tf, self.x_r_tf)\n",
        "        \n",
        "        # Define predictions for NTK computation\n",
        "        self.u_ntk_pred = self.net_u(self.t_u_ntk_tf, self.x_u_ntk_tf)\n",
        "        self.ut_ntk_pred = self.net_u_t(self.t_ut_ntk_tf, self.x_ut_ntk_tf)\n",
        "        self.r_ntk_pred = self.net_r(self.t_r_ntk_tf, self.x_r_ntk_tf)\n",
        "\n",
        "        # Boundary loss and Initial loss\n",
        "        self.loss_ics_u = tf.reduce_mean(tf.square(self.u_ics_tf - self.u_ics_pred))\n",
        "        self.loss_ics_u_t = tf.reduce_mean(tf.square(self.u_t_ics_pred))\n",
        "        self.loss_bc1 = tf.reduce_mean(tf.square(self.u_bc1_pred))\n",
        "        self.loss_bc2 = tf.reduce_mean(tf.square(self.u_bc2_pred))\n",
        "\n",
        "        self.loss_bcs = self.loss_ics_u + self.loss_bc1 + self.loss_bc2\n",
        "\n",
        "        # Residual loss\n",
        "        self.loss_res = tf.reduce_mean(tf.square(self.r_pred))\n",
        "\n",
        "        # Total loss\n",
        "        self.loss = self.loss_res +  self.loss_bcs +self.loss_ics_u_t \n",
        "\n",
        "        # Define optimizer with learning rate schedule\n",
        "        self.global_step = tf.Variable(0, trainable=False)\n",
        "        starter_learning_rate = 1e-3\n",
        "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step,  1000, 0.9, staircase=False)\n",
        "        # Passing global_step to minimize() will increment it at each step.\n",
        "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
        "\n",
        "        # # Logger\n",
        "        # self.loss_bcs_log = []\n",
        "        # self.loss_ut_ics_log = []\n",
        "        # self.loss_res_log = []\n",
        "        # self.saver = tf.train.Saver()\n",
        "        \n",
        "        # # Compute the Jacobian for weights and biases in each hidden layer  \n",
        "        # self.J_u = self.compute_jacobian(self.u_ntk_pred)\n",
        "        # self.J_ut = self.compute_jacobian(self.ut_ntk_pred)\n",
        "        # self.J_r = self.compute_jacobian(self.r_ntk_pred)\n",
        "        \n",
        "        # self.K_u = self.compute_ntk(self.J_u, D1, self.J_u, D1)\n",
        "        # self.K_ut = self.compute_ntk(self.J_ut, D2, self.J_ut, D2)\n",
        "        # self.K_r = self.compute_ntk(self.J_r, D3, self.J_r, D3)\n",
        "        \n",
        "        # # NTK logger \n",
        "        # self.K_u_log = []\n",
        "        # self.K_ut_log = []\n",
        "        # self.K_r_log = []\n",
        "        \n",
        "        # # weights logger\n",
        "        # self.lam_u_log = []\n",
        "        # self.lam_ut_log = []\n",
        "        # self.lam_r_log = []\n",
        "        \n",
        "         # Initialize Tensorflow variables\n",
        "        init = tf.global_variables_initializer()\n",
        "        self.sess.run(init)\n",
        "\n",
        "    # Initialize network weights and biases using Xavier initialization\n",
        "    def initialize_NN(self, layers):\n",
        "        # Xavier initialization\n",
        "        def xavier_init(size):\n",
        "            in_dim = size[0]\n",
        "            out_dim = size[1]\n",
        "            xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
        "            return tf.Variable(tf.random.normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev,  dtype=tf.float32)\n",
        "\n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0, num_layers - 1):\n",
        "            W = xavier_init(size=[layers[l], layers[l + 1]])\n",
        "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
        "            weights.append(W)\n",
        "            biases.append(b)\n",
        "        return weights, biases\n",
        "\n",
        "    # Evaluates the forward pass\n",
        "    def forward_pass(self, H, layers, weights, biases):\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0, num_layers - 2):\n",
        "            W = weights[l]\n",
        "            b = biases[l]\n",
        "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
        "        W = weights[-1]\n",
        "        b = biases[-1]\n",
        "        H = tf.add(tf.matmul(H, W), b)\n",
        "        return H\n",
        "\n",
        "    # Forward pass for u\n",
        "    def net_u(self, t, x):\n",
        "        u = self.forward_pass(tf.concat([t, x], 1),\n",
        "                              self.layers,\n",
        "                              self.weights,\n",
        "                              self.biases)\n",
        "        return u\n",
        "\n",
        "    # Forward pass for du/dt\n",
        "    def net_u_t(self, t, x):\n",
        "        u_t = tf.gradients(self.net_u(t, x), t)[0] / self.sigma_t\n",
        "        return u_t\n",
        "\n",
        "    # Forward pass for the residual\n",
        "    def net_r(self, t, x):\n",
        "        u = self.net_u(t, x)\n",
        "        residual = self.operator(u, t, x,\n",
        "                                 self.c,\n",
        "                                 self.sigma_t,\n",
        "                                 self.sigma_x)\n",
        "        return residual\n",
        "    \n",
        "    \n",
        "    # # Compute Jacobian for each weights and biases in each layer and retrun a list \n",
        "    # def compute_jacobian(self, f):\n",
        "    #     J_list =[]\n",
        "    #     L = len(self.weights)    \n",
        "    #     for i in range(L):\n",
        "    #         J_w = jacobian(f, self.weights[i])\n",
        "    #         J_list.append(J_w)\n",
        "     \n",
        "    #     for i in range(L):\n",
        "    #         J_b = jacobian(f, self.biases[i])\n",
        "    #         J_list.append(J_b)\n",
        "    #     return J_list\n",
        "    \n",
        "    # # Compute the empirical NTK = J J^T\n",
        "    # def compute_ntk(self, J1_list, D1, J2_list, D2):\n",
        "\n",
        "    #     N = len(J1_list)\n",
        "        \n",
        "    #     Ker = tf.zeros((D1,D2))\n",
        "    #     for k in range(N):\n",
        "    #         J1 = tf.reshape(J1_list[k], shape=(D1,-1))\n",
        "    #         J2 = tf.reshape(J2_list[k], shape=(D2,-1))\n",
        "            \n",
        "    #         K = tf.matmul(J1, tf.transpose(J2))\n",
        "    #         Ker = Ker + K\n",
        "    #     return Ker\n",
        "\n",
        "    def fetch_minibatch(self, sampler, N):\n",
        "        X, Y = sampler.sample(N)\n",
        "        X = (X - self.mu_X) / self.sigma_X\n",
        "        return X, Y\n",
        "\n",
        "        # Trains the model by minimizing the MSE loss\n",
        "\n",
        "    def trainmb(self, nIter=10000, batch_size=128, log_NTK=False, update_lam=False):\n",
        "\n",
        "        start_time = timeit.default_timer()\n",
        "        for it in range(nIter):\n",
        "            # Fetch boundary mini-batches\n",
        "            X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, batch_size // 3)\n",
        "            X_bc1_batch, _ = self.fetch_minibatch(self.bcs_sampler[0], batch_size // 3)\n",
        "            X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], batch_size // 3)\n",
        "            \n",
        "            # Fetch residual mini-batch\n",
        "            X_res_batch, _ = self.fetch_minibatch(self.res_sampler, batch_size)\n",
        "            # Define a dictionary for associating placeholders with data\n",
        "            tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1], self.x_ics_tf: X_ics_batch[:, 1:2],\n",
        "                       self.u_ics_tf: u_ics_batch,\n",
        "                       self.t_bc1_tf: X_bc1_batch[:, 0:1], self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
        "                       self.t_bc2_tf: X_bc2_batch[:, 0:1], self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
        "                       self.t_r_tf: X_res_batch[:, 0:1], self.x_r_tf: X_res_batch[:, 1:2]}\n",
        "\n",
        "            # Run the Tensorflow session to minimize the loss\n",
        "            self.sess.run(self.train_op, tf_dict)\n",
        "\n",
        "            # Print\n",
        "            if it % 100 == 0:\n",
        "                elapsed = timeit.default_timer() - start_time\n",
        "\n",
        "                loss_value = self.sess.run(self.loss, tf_dict)\n",
        "                loss_bcs_value = self.sess.run(self.loss_bcs, tf_dict)\n",
        "                loss_ics_ut_value = self.sess.run(self.loss_ics_u_t, tf_dict)\n",
        "                loss_res_value = self.sess.run(self.loss_res, tf_dict)\n",
        "\n",
        "\n",
        "                print('It: %d, Loss: %.3e, Loss_res: %.3e,  Loss_bcs: %.3e, Loss_ut_ics: %.3e,, Time: %.2f' %(it, loss_value, loss_res_value, loss_bcs_value, loss_ics_ut_value, elapsed))\n",
        "                \n",
        " \n",
        "                sys.stdout.flush()\n",
        "                start_time = timeit.default_timer()\n",
        "            \n",
        "  \n",
        "                        \n",
        "    def train(self, nIter , bcbatch_size , ubatch_size):\n",
        "\n",
        "        start_time = timeit.default_timer()\n",
        "\n",
        "        # Fetch boundary mini-batches\n",
        "        X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, bcbatch_size)\n",
        "        X_bc1_batch, _ = self.fetch_minibatch(self.bcs_sampler[0], bcbatch_size )\n",
        "        X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], bcbatch_size )\n",
        "        \n",
        "        # Fetch residual mini-batch\n",
        "        X_res_batch, _ = self.fetch_minibatch(self.res_sampler, ubatch_size)\n",
        "        # print(\"inside trainmb: \" , X_res_batch.shape)\n",
        "        # Define a dictionary for associating placeholders with data\n",
        "        tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1], self.x_ics_tf: X_ics_batch[:, 1:2],\n",
        "                    self.u_ics_tf: u_ics_batch,\n",
        "                    self.t_bc1_tf: X_bc1_batch[:, 0:1], self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
        "                    self.t_bc2_tf: X_bc2_batch[:, 0:1], self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
        "                    self.t_r_tf: X_res_batch[:, 0:1], self.x_r_tf: X_res_batch[:, 1:2]\n",
        "                    }\n",
        "        \n",
        "     \n",
        "        for it in range(nIter):\n",
        "\n",
        "            # Run the Tensorflow session to minimize the loss\n",
        "            self.sess.run(self.train_op, tf_dict)\n",
        "\n",
        "            # Print\n",
        "            if it % 100 == 0:\n",
        "                elapsed = timeit.default_timer() - start_time\n",
        "\n",
        "                loss_value = self.sess.run(self.loss, tf_dict)\n",
        "                loss_bcs_value = self.sess.run(self.loss_bcs, tf_dict)\n",
        "                loss_ics_ut_value = self.sess.run(self.loss_ics_u_t, tf_dict)\n",
        "                loss_res_value = self.sess.run(self.loss_res, tf_dict)\n",
        "\n",
        "              \n",
        "\n",
        "                print('It: %d, Loss: %.3e, Loss_res: %.3e,  Loss_bcs: %.3e, Loss_ut_ics: %.3e,, Time: %.2f' %(it, loss_value, loss_res_value, loss_bcs_value, loss_ics_ut_value, elapsed))\n",
        "                \n",
        "          \n",
        "                sys.stdout.flush()\n",
        "                start_time = timeit.default_timer()\n",
        "    \n",
        "    # Evaluates predictions at test points\n",
        "    def predict_u(self, X_star):\n",
        "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
        "        tf_dict = {self.t_u_tf: X_star[:, 0:1], self.x_u_tf: X_star[:, 1:2]}\n",
        "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
        "        return u_star\n",
        "\n",
        "        # Evaluates predictions at test points\n",
        "\n",
        "    def predict_r(self, X_star):\n",
        "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
        "        tf_dict = {self.t_r_tf: X_star[:, 0:1], self.x_r_tf: X_star[:, 1:2]}\n",
        "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
        "        return r_star\n",
        "    \n",
        "   ###############################################################################################################################################\n",
        "   # \n",
        "   # ###############################################################################################################################################\n",
        "   # \n",
        "   # ###############################################################################################################################################\n",
        "   # \n",
        "   #  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#test_method(mtd , layers,  X_u, Y_u, X_r, Y_r ,  X_star , u_star , r_star  , nIter ,batch_size , bcbatch_size , ubatch_size)\n",
        "def test_method(method , layers,  ics_sampler, bcs_sampler, res_sampler, c ,kernel_size , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size ):\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
        "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
        "        # sess.run(init)\n",
        "\n",
        "        model = PINN(layers, operator, ics_sampler, bcs_sampler, res_sampler, c, kernel_size , sess)\n",
        "        # Train model\n",
        "        start_time = time.time()\n",
        "\n",
        "        if method ==\"full_batch\":\n",
        "            print(\"full_batch method is used\")\n",
        "            model.train(nIter  , bcbatch_size , ubatch_size  )\n",
        "        elif method ==\"mini_batch\":\n",
        "            print(\"mini_batch method is used\")\n",
        "            model.trainmb(nIter, mbbatch_size)\n",
        "        else:\n",
        "            print(\"unknown method!\")\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        # Predictions\n",
        "        u_pred = model.predict_u(X_star)\n",
        "        r_pred = model.predict_r(X_star)\n",
        "        # Predictions\n",
        "\n",
        "        sess.close()   \n",
        "\n",
        "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "\n",
        "    print('elapsed: {:.2e}'.format(elapsed))\n",
        "\n",
        "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "\n",
        "\n",
        "    return [elapsed, error_u  ]\n",
        "\n",
        "###############################################################################################################################################\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YGFobW0EatXj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Method:  mini_batch\n",
            "Epoch:  1\n",
            "WARNING:tensorflow:From /tmp/ipykernel_17025/187544903.py:4: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_17025/187544903.py:5: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_17025/187544903.py:6: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_17025/187544903.py:6: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_17025/2656214194.py:35: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-25 18:19:04.953772: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
            "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-25 18:19:04.981252: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
            "2023-11-25 18:19:04.981707: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560119d2c590 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2023-11-25 18:19:04.981719: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2023-11-25 18:19:04.982107: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tmp/ipykernel_17025/2656214194.py:101: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_17025/2656214194.py:103: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_17025/2656214194.py:131: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "mini_batch method is used\n",
            "It: 0, Loss: 2.441e+01, Loss_res: 6.696e-01,  Loss_bcs: 6.171e+00, Loss_ut_ics: 1.757e+01,, Time: 1.56\n",
            "It: 100, Loss: 4.510e-01, Loss_res: 7.240e-05,  Loss_bcs: 4.280e-01, Loss_ut_ics: 2.291e-02,, Time: 3.12\n",
            "It: 200, Loss: 3.786e-01, Loss_res: 1.507e-03,  Loss_bcs: 3.619e-01, Loss_ut_ics: 1.516e-02,, Time: 2.91\n",
            "It: 300, Loss: 3.289e-01, Loss_res: 6.680e-03,  Loss_bcs: 3.209e-01, Loss_ut_ics: 1.349e-03,, Time: 3.09\n",
            "It: 400, Loss: 3.881e-01, Loss_res: 4.829e-02,  Loss_bcs: 3.372e-01, Loss_ut_ics: 2.608e-03,, Time: 2.96\n",
            "It: 500, Loss: 2.695e-01, Loss_res: 1.457e-02,  Loss_bcs: 2.513e-01, Loss_ut_ics: 3.618e-03,, Time: 2.93\n",
            "It: 600, Loss: 2.458e-01, Loss_res: 5.850e-03,  Loss_bcs: 2.303e-01, Loss_ut_ics: 9.619e-03,, Time: 2.97\n",
            "It: 700, Loss: 2.731e-01, Loss_res: 1.396e-02,  Loss_bcs: 2.497e-01, Loss_ut_ics: 9.429e-03,, Time: 3.01\n",
            "It: 800, Loss: 2.242e-01, Loss_res: 4.567e-03,  Loss_bcs: 2.164e-01, Loss_ut_ics: 3.233e-03,, Time: 2.99\n",
            "It: 900, Loss: 2.553e-01, Loss_res: 2.710e-02,  Loss_bcs: 2.165e-01, Loss_ut_ics: 1.177e-02,, Time: 3.08\n",
            "It: 1000, Loss: 2.045e-01, Loss_res: 9.771e-03,  Loss_bcs: 1.820e-01, Loss_ut_ics: 1.279e-02,, Time: 3.21\n",
            "It: 1100, Loss: 2.047e-01, Loss_res: 3.732e-03,  Loss_bcs: 1.983e-01, Loss_ut_ics: 2.708e-03,, Time: 3.03\n",
            "It: 1200, Loss: 1.850e-01, Loss_res: 7.314e-03,  Loss_bcs: 1.766e-01, Loss_ut_ics: 1.123e-03,, Time: 3.02\n",
            "It: 1300, Loss: 2.127e-01, Loss_res: 1.271e-02,  Loss_bcs: 1.886e-01, Loss_ut_ics: 1.140e-02,, Time: 3.15\n",
            "It: 1400, Loss: 1.806e-01, Loss_res: 2.129e-02,  Loss_bcs: 1.563e-01, Loss_ut_ics: 2.971e-03,, Time: 3.02\n",
            "It: 1500, Loss: 1.967e-01, Loss_res: 4.151e-02,  Loss_bcs: 1.529e-01, Loss_ut_ics: 2.284e-03,, Time: 3.16\n",
            "It: 1600, Loss: 1.792e-01, Loss_res: 4.151e-03,  Loss_bcs: 1.694e-01, Loss_ut_ics: 5.585e-03,, Time: 3.51\n",
            "It: 1700, Loss: 1.702e-01, Loss_res: 1.485e-02,  Loss_bcs: 1.495e-01, Loss_ut_ics: 5.813e-03,, Time: 3.33\n",
            "It: 1800, Loss: 1.526e-01, Loss_res: 4.252e-03,  Loss_bcs: 1.445e-01, Loss_ut_ics: 3.837e-03,, Time: 3.24\n",
            "It: 1900, Loss: 1.894e-01, Loss_res: 2.335e-02,  Loss_bcs: 1.590e-01, Loss_ut_ics: 7.127e-03,, Time: 3.45\n",
            "It: 2000, Loss: 1.594e-01, Loss_res: 8.724e-03,  Loss_bcs: 1.396e-01, Loss_ut_ics: 1.108e-02,, Time: 3.02\n",
            "It: 2100, Loss: 2.010e-01, Loss_res: 1.309e-02,  Loss_bcs: 1.780e-01, Loss_ut_ics: 9.891e-03,, Time: 3.09\n",
            "It: 2200, Loss: 2.273e-01, Loss_res: 6.734e-02,  Loss_bcs: 1.484e-01, Loss_ut_ics: 1.147e-02,, Time: 3.21\n",
            "It: 2300, Loss: 1.485e-01, Loss_res: 8.587e-03,  Loss_bcs: 1.386e-01, Loss_ut_ics: 1.295e-03,, Time: 3.06\n",
            "It: 2400, Loss: 1.651e-01, Loss_res: 3.615e-03,  Loss_bcs: 1.493e-01, Loss_ut_ics: 1.220e-02,, Time: 3.24\n",
            "It: 2500, Loss: 1.441e-01, Loss_res: 8.565e-03,  Loss_bcs: 1.310e-01, Loss_ut_ics: 4.524e-03,, Time: 3.37\n",
            "It: 2600, Loss: 1.758e-01, Loss_res: 3.760e-03,  Loss_bcs: 1.614e-01, Loss_ut_ics: 1.070e-02,, Time: 3.83\n",
            "It: 2700, Loss: 1.631e-01, Loss_res: 3.224e-03,  Loss_bcs: 1.490e-01, Loss_ut_ics: 1.092e-02,, Time: 3.38\n",
            "It: 2800, Loss: 1.710e-01, Loss_res: 1.630e-02,  Loss_bcs: 1.522e-01, Loss_ut_ics: 2.511e-03,, Time: 3.30\n",
            "It: 2900, Loss: 1.476e-01, Loss_res: 6.481e-03,  Loss_bcs: 1.378e-01, Loss_ut_ics: 3.393e-03,, Time: 3.45\n",
            "It: 3000, Loss: 1.659e-01, Loss_res: 1.285e-02,  Loss_bcs: 1.454e-01, Loss_ut_ics: 7.597e-03,, Time: 3.55\n",
            "It: 3100, Loss: 1.864e-01, Loss_res: 1.974e-02,  Loss_bcs: 1.506e-01, Loss_ut_ics: 1.605e-02,, Time: 3.59\n",
            "It: 3200, Loss: 1.678e-01, Loss_res: 3.830e-03,  Loss_bcs: 1.579e-01, Loss_ut_ics: 6.141e-03,, Time: 3.81\n",
            "It: 3300, Loss: 1.633e-01, Loss_res: 5.009e-03,  Loss_bcs: 1.442e-01, Loss_ut_ics: 1.409e-02,, Time: 4.01\n",
            "It: 3400, Loss: 1.484e-01, Loss_res: 2.109e-03,  Loss_bcs: 1.447e-01, Loss_ut_ics: 1.587e-03,, Time: 4.06\n",
            "It: 3500, Loss: 1.755e-01, Loss_res: 3.723e-02,  Loss_bcs: 1.303e-01, Loss_ut_ics: 7.951e-03,, Time: 3.98\n",
            "It: 3600, Loss: 1.725e-01, Loss_res: 4.634e-03,  Loss_bcs: 1.606e-01, Loss_ut_ics: 7.306e-03,, Time: 3.70\n",
            "It: 3700, Loss: 1.732e-01, Loss_res: 5.324e-03,  Loss_bcs: 1.640e-01, Loss_ut_ics: 3.941e-03,, Time: 3.70\n",
            "It: 3800, Loss: 1.418e-01, Loss_res: 6.416e-03,  Loss_bcs: 1.333e-01, Loss_ut_ics: 2.042e-03,, Time: 3.96\n",
            "It: 3900, Loss: 1.371e-01, Loss_res: 2.604e-03,  Loss_bcs: 1.326e-01, Loss_ut_ics: 1.931e-03,, Time: 11.57\n",
            "It: 4000, Loss: 1.513e-01, Loss_res: 3.893e-03,  Loss_bcs: 1.451e-01, Loss_ut_ics: 2.253e-03,, Time: 7.82\n",
            "It: 4100, Loss: 1.875e-01, Loss_res: 3.615e-02,  Loss_bcs: 1.441e-01, Loss_ut_ics: 7.200e-03,, Time: 9.57\n",
            "It: 4200, Loss: 1.432e-01, Loss_res: 1.557e-03,  Loss_bcs: 1.380e-01, Loss_ut_ics: 3.666e-03,, Time: 7.98\n",
            "It: 4300, Loss: 1.669e-01, Loss_res: 3.673e-03,  Loss_bcs: 1.602e-01, Loss_ut_ics: 3.055e-03,, Time: 9.11\n",
            "It: 4400, Loss: 1.447e-01, Loss_res: 3.487e-03,  Loss_bcs: 1.374e-01, Loss_ut_ics: 3.772e-03,, Time: 8.21\n",
            "It: 4500, Loss: 1.785e-01, Loss_res: 2.136e-02,  Loss_bcs: 1.517e-01, Loss_ut_ics: 5.373e-03,, Time: 9.54\n",
            "It: 4600, Loss: 1.656e-01, Loss_res: 2.475e-02,  Loss_bcs: 1.330e-01, Loss_ut_ics: 7.799e-03,, Time: 8.59\n",
            "It: 4700, Loss: 1.528e-01, Loss_res: 2.509e-03,  Loss_bcs: 1.484e-01, Loss_ut_ics: 1.816e-03,, Time: 8.75\n",
            "It: 4800, Loss: 1.513e-01, Loss_res: 2.689e-03,  Loss_bcs: 1.451e-01, Loss_ut_ics: 3.553e-03,, Time: 12.91\n",
            "It: 4900, Loss: 1.418e-01, Loss_res: 5.446e-03,  Loss_bcs: 1.347e-01, Loss_ut_ics: 1.646e-03,, Time: 12.28\n",
            "It: 5000, Loss: 1.499e-01, Loss_res: 8.715e-03,  Loss_bcs: 1.393e-01, Loss_ut_ics: 1.886e-03,, Time: 11.67\n",
            "It: 5100, Loss: 1.702e-01, Loss_res: 5.783e-03,  Loss_bcs: 1.519e-01, Loss_ut_ics: 1.247e-02,, Time: 10.41\n",
            "It: 5200, Loss: 1.551e-01, Loss_res: 6.833e-03,  Loss_bcs: 1.445e-01, Loss_ut_ics: 3.783e-03,, Time: 9.08\n",
            "It: 5300, Loss: 1.578e-01, Loss_res: 2.930e-02,  Loss_bcs: 1.262e-01, Loss_ut_ics: 2.371e-03,, Time: 10.11\n",
            "It: 5400, Loss: 1.537e-01, Loss_res: 5.103e-03,  Loss_bcs: 1.467e-01, Loss_ut_ics: 1.919e-03,, Time: 7.68\n",
            "It: 5500, Loss: 1.557e-01, Loss_res: 1.304e-02,  Loss_bcs: 1.411e-01, Loss_ut_ics: 1.512e-03,, Time: 10.36\n",
            "It: 5600, Loss: 1.439e-01, Loss_res: 1.972e-03,  Loss_bcs: 1.394e-01, Loss_ut_ics: 2.537e-03,, Time: 7.94\n",
            "It: 5700, Loss: 1.552e-01, Loss_res: 4.637e-03,  Loss_bcs: 1.483e-01, Loss_ut_ics: 2.255e-03,, Time: 10.38\n",
            "It: 5800, Loss: 1.548e-01, Loss_res: 6.811e-03,  Loss_bcs: 1.466e-01, Loss_ut_ics: 1.464e-03,, Time: 10.43\n",
            "It: 5900, Loss: 1.494e-01, Loss_res: 5.872e-03,  Loss_bcs: 1.392e-01, Loss_ut_ics: 4.288e-03,, Time: 9.78\n",
            "It: 6000, Loss: 1.590e-01, Loss_res: 5.206e-03,  Loss_bcs: 1.497e-01, Loss_ut_ics: 4.129e-03,, Time: 10.93\n",
            "It: 6100, Loss: 1.559e-01, Loss_res: 2.577e-03,  Loss_bcs: 1.471e-01, Loss_ut_ics: 6.229e-03,, Time: 8.21\n",
            "It: 6200, Loss: 1.627e-01, Loss_res: 7.665e-03,  Loss_bcs: 1.525e-01, Loss_ut_ics: 2.595e-03,, Time: 10.18\n",
            "It: 6300, Loss: 1.399e-01, Loss_res: 3.937e-03,  Loss_bcs: 1.297e-01, Loss_ut_ics: 6.230e-03,, Time: 8.34\n",
            "It: 6400, Loss: 1.378e-01, Loss_res: 1.446e-02,  Loss_bcs: 1.224e-01, Loss_ut_ics: 9.725e-04,, Time: 9.50\n",
            "It: 6500, Loss: 1.549e-01, Loss_res: 1.340e-02,  Loss_bcs: 1.365e-01, Loss_ut_ics: 5.040e-03,, Time: 9.67\n",
            "It: 6600, Loss: 1.493e-01, Loss_res: 1.543e-03,  Loss_bcs: 1.454e-01, Loss_ut_ics: 2.376e-03,, Time: 8.90\n",
            "It: 6700, Loss: 1.579e-01, Loss_res: 2.651e-03,  Loss_bcs: 1.526e-01, Loss_ut_ics: 2.588e-03,, Time: 10.39\n",
            "It: 6800, Loss: 1.236e-01, Loss_res: 5.615e-03,  Loss_bcs: 1.169e-01, Loss_ut_ics: 1.045e-03,, Time: 7.85\n",
            "It: 6900, Loss: 1.372e-01, Loss_res: 1.663e-03,  Loss_bcs: 1.349e-01, Loss_ut_ics: 6.363e-04,, Time: 12.02\n",
            "It: 7000, Loss: 1.516e-01, Loss_res: 4.427e-03,  Loss_bcs: 1.457e-01, Loss_ut_ics: 1.442e-03,, Time: 9.93\n",
            "It: 7100, Loss: 1.347e-01, Loss_res: 5.943e-03,  Loss_bcs: 1.273e-01, Loss_ut_ics: 1.381e-03,, Time: 10.10\n",
            "It: 7200, Loss: 1.330e-01, Loss_res: 4.304e-03,  Loss_bcs: 1.276e-01, Loss_ut_ics: 1.140e-03,, Time: 9.36\n",
            "It: 7300, Loss: 1.154e-01, Loss_res: 2.799e-03,  Loss_bcs: 1.116e-01, Loss_ut_ics: 1.085e-03,, Time: 8.85\n",
            "It: 7400, Loss: 1.372e-01, Loss_res: 6.846e-03,  Loss_bcs: 1.269e-01, Loss_ut_ics: 3.400e-03,, Time: 10.02\n",
            "It: 7500, Loss: 1.463e-01, Loss_res: 1.164e-02,  Loss_bcs: 1.282e-01, Loss_ut_ics: 6.509e-03,, Time: 8.01\n",
            "It: 7600, Loss: 1.378e-01, Loss_res: 1.204e-02,  Loss_bcs: 1.252e-01, Loss_ut_ics: 5.832e-04,, Time: 10.77\n",
            "It: 7700, Loss: 1.389e-01, Loss_res: 4.907e-03,  Loss_bcs: 1.335e-01, Loss_ut_ics: 4.386e-04,, Time: 7.59\n",
            "It: 7800, Loss: 1.304e-01, Loss_res: 6.191e-03,  Loss_bcs: 1.231e-01, Loss_ut_ics: 1.125e-03,, Time: 10.59\n",
            "It: 7900, Loss: 1.665e-01, Loss_res: 3.536e-02,  Loss_bcs: 1.298e-01, Loss_ut_ics: 1.270e-03,, Time: 8.78\n",
            "It: 8000, Loss: 1.443e-01, Loss_res: 1.617e-02,  Loss_bcs: 1.273e-01, Loss_ut_ics: 7.618e-04,, Time: 10.10\n",
            "It: 8100, Loss: 1.514e-01, Loss_res: 1.082e-02,  Loss_bcs: 1.296e-01, Loss_ut_ics: 1.095e-02,, Time: 8.50\n",
            "It: 8200, Loss: 1.314e-01, Loss_res: 6.070e-03,  Loss_bcs: 1.246e-01, Loss_ut_ics: 7.745e-04,, Time: 9.66\n",
            "It: 8300, Loss: 1.219e-01, Loss_res: 5.180e-03,  Loss_bcs: 1.150e-01, Loss_ut_ics: 1.686e-03,, Time: 9.68\n",
            "It: 8400, Loss: 1.199e-01, Loss_res: 6.501e-03,  Loss_bcs: 1.123e-01, Loss_ut_ics: 1.103e-03,, Time: 8.73\n",
            "It: 8500, Loss: 1.184e-01, Loss_res: 2.550e-03,  Loss_bcs: 1.145e-01, Loss_ut_ics: 1.297e-03,, Time: 10.01\n",
            "It: 8600, Loss: 1.223e-01, Loss_res: 4.449e-03,  Loss_bcs: 1.149e-01, Loss_ut_ics: 2.941e-03,, Time: 7.77\n",
            "It: 8700, Loss: 1.218e-01, Loss_res: 2.368e-03,  Loss_bcs: 1.184e-01, Loss_ut_ics: 9.686e-04,, Time: 10.32\n",
            "It: 8800, Loss: 1.449e-01, Loss_res: 2.276e-03,  Loss_bcs: 1.418e-01, Loss_ut_ics: 8.335e-04,, Time: 7.49\n",
            "It: 8900, Loss: 1.519e-01, Loss_res: 5.161e-02,  Loss_bcs: 9.604e-02, Loss_ut_ics: 4.256e-03,, Time: 10.55\n",
            "It: 9000, Loss: 1.551e-01, Loss_res: 3.638e-03,  Loss_bcs: 1.509e-01, Loss_ut_ics: 5.806e-04,, Time: 7.99\n",
            "It: 9100, Loss: 1.275e-01, Loss_res: 2.842e-03,  Loss_bcs: 1.243e-01, Loss_ut_ics: 3.348e-04,, Time: 11.30\n",
            "It: 9200, Loss: 1.228e-01, Loss_res: 1.646e-03,  Loss_bcs: 1.127e-01, Loss_ut_ics: 8.448e-03,, Time: 9.07\n",
            "It: 9300, Loss: 1.094e-01, Loss_res: 5.643e-03,  Loss_bcs: 1.025e-01, Loss_ut_ics: 1.259e-03,, Time: 8.94\n",
            "It: 9400, Loss: 1.067e-01, Loss_res: 8.228e-03,  Loss_bcs: 9.765e-02, Loss_ut_ics: 8.127e-04,, Time: 10.02\n",
            "It: 9500, Loss: 1.065e-01, Loss_res: 1.479e-03,  Loss_bcs: 1.041e-01, Loss_ut_ics: 9.877e-04,, Time: 8.46\n",
            "It: 9600, Loss: 1.810e-01, Loss_res: 6.574e-02,  Loss_bcs: 1.121e-01, Loss_ut_ics: 3.147e-03,, Time: 9.86\n",
            "It: 9700, Loss: 1.168e-01, Loss_res: 4.682e-03,  Loss_bcs: 1.113e-01, Loss_ut_ics: 8.361e-04,, Time: 7.50\n",
            "It: 9800, Loss: 1.156e-01, Loss_res: 3.238e-03,  Loss_bcs: 1.104e-01, Loss_ut_ics: 2.045e-03,, Time: 10.10\n",
            "It: 9900, Loss: 2.783e-01, Loss_res: 1.666e-01,  Loss_bcs: 1.027e-01, Loss_ut_ics: 9.024e-03,, Time: 7.39\n",
            "It: 10000, Loss: 1.097e-01, Loss_res: 1.531e-03,  Loss_bcs: 1.076e-01, Loss_ut_ics: 5.666e-04,, Time: 9.85\n",
            "It: 10100, Loss: 1.177e-01, Loss_res: 1.584e-03,  Loss_bcs: 1.150e-01, Loss_ut_ics: 1.107e-03,, Time: 7.47\n",
            "It: 10200, Loss: 1.191e-01, Loss_res: 2.104e-03,  Loss_bcs: 1.146e-01, Loss_ut_ics: 2.344e-03,, Time: 9.79\n",
            "It: 10300, Loss: 1.204e-01, Loss_res: 2.382e-03,  Loss_bcs: 1.061e-01, Loss_ut_ics: 1.184e-02,, Time: 7.75\n",
            "It: 10400, Loss: 9.802e-02, Loss_res: 5.998e-03,  Loss_bcs: 9.029e-02, Loss_ut_ics: 1.740e-03,, Time: 9.33\n",
            "It: 10500, Loss: 4.245e-01, Loss_res: 3.102e-01,  Loss_bcs: 1.045e-01, Loss_ut_ics: 9.880e-03,, Time: 7.87\n",
            "It: 10600, Loss: 1.025e-01, Loss_res: 1.148e-03,  Loss_bcs: 9.941e-02, Loss_ut_ics: 1.890e-03,, Time: 9.15\n",
            "It: 10700, Loss: 1.084e-01, Loss_res: 1.495e-03,  Loss_bcs: 1.050e-01, Loss_ut_ics: 1.861e-03,, Time: 8.11\n",
            "It: 10800, Loss: 1.038e-01, Loss_res: 8.460e-04,  Loss_bcs: 1.015e-01, Loss_ut_ics: 1.480e-03,, Time: 8.74\n",
            "It: 10900, Loss: 1.022e-01, Loss_res: 1.671e-03,  Loss_bcs: 9.892e-02, Loss_ut_ics: 1.606e-03,, Time: 8.49\n",
            "It: 11000, Loss: 1.026e-01, Loss_res: 1.810e-03,  Loss_bcs: 9.803e-02, Loss_ut_ics: 2.749e-03,, Time: 8.51\n",
            "It: 11100, Loss: 9.987e-02, Loss_res: 1.875e-03,  Loss_bcs: 9.627e-02, Loss_ut_ics: 1.731e-03,, Time: 8.82\n",
            "It: 11200, Loss: 1.410e-01, Loss_res: 4.037e-02,  Loss_bcs: 9.700e-02, Loss_ut_ics: 3.637e-03,, Time: 8.02\n",
            "It: 11300, Loss: 8.731e-02, Loss_res: 1.516e-03,  Loss_bcs: 8.462e-02, Loss_ut_ics: 1.166e-03,, Time: 9.13\n",
            "It: 11400, Loss: 1.069e-01, Loss_res: 2.096e-03,  Loss_bcs: 1.029e-01, Loss_ut_ics: 1.954e-03,, Time: 7.82\n",
            "It: 11500, Loss: 8.227e-02, Loss_res: 1.187e-03,  Loss_bcs: 7.947e-02, Loss_ut_ics: 1.607e-03,, Time: 9.77\n",
            "It: 11600, Loss: 1.068e-01, Loss_res: 1.643e-03,  Loss_bcs: 1.009e-01, Loss_ut_ics: 4.257e-03,, Time: 7.21\n",
            "It: 11700, Loss: 1.085e-01, Loss_res: 4.793e-03,  Loss_bcs: 9.898e-02, Loss_ut_ics: 4.690e-03,, Time: 9.92\n",
            "It: 11800, Loss: 9.072e-02, Loss_res: 2.424e-03,  Loss_bcs: 8.608e-02, Loss_ut_ics: 2.212e-03,, Time: 7.11\n",
            "It: 11900, Loss: 1.077e-01, Loss_res: 3.043e-03,  Loss_bcs: 9.745e-02, Loss_ut_ics: 7.246e-03,, Time: 9.81\n",
            "It: 12000, Loss: 3.065e-01, Loss_res: 1.854e-01,  Loss_bcs: 9.377e-02, Loss_ut_ics: 2.734e-02,, Time: 7.37\n",
            "It: 12100, Loss: 8.935e-02, Loss_res: 8.646e-04,  Loss_bcs: 8.481e-02, Loss_ut_ics: 3.671e-03,, Time: 9.51\n",
            "It: 12200, Loss: 9.362e-02, Loss_res: 2.849e-03,  Loss_bcs: 8.883e-02, Loss_ut_ics: 1.940e-03,, Time: 7.67\n",
            "It: 12300, Loss: 9.720e-02, Loss_res: 5.490e-03,  Loss_bcs: 8.744e-02, Loss_ut_ics: 4.275e-03,, Time: 9.45\n",
            "It: 12400, Loss: 1.028e-01, Loss_res: 3.414e-03,  Loss_bcs: 9.184e-02, Loss_ut_ics: 7.519e-03,, Time: 7.81\n",
            "It: 12500, Loss: 1.035e-01, Loss_res: 3.991e-03,  Loss_bcs: 9.555e-02, Loss_ut_ics: 3.948e-03,, Time: 9.06\n",
            "It: 12600, Loss: 9.601e-02, Loss_res: 7.057e-03,  Loss_bcs: 8.621e-02, Loss_ut_ics: 2.745e-03,, Time: 8.02\n",
            "It: 12700, Loss: 8.433e-02, Loss_res: 2.537e-03,  Loss_bcs: 7.881e-02, Loss_ut_ics: 2.983e-03,, Time: 8.95\n",
            "It: 12800, Loss: 9.206e-02, Loss_res: 1.294e-02,  Loss_bcs: 7.553e-02, Loss_ut_ics: 3.593e-03,, Time: 8.25\n",
            "It: 12900, Loss: 9.569e-02, Loss_res: 4.973e-03,  Loss_bcs: 8.679e-02, Loss_ut_ics: 3.927e-03,, Time: 8.56\n",
            "It: 13000, Loss: 8.347e-02, Loss_res: 2.059e-03,  Loss_bcs: 7.881e-02, Loss_ut_ics: 2.600e-03,, Time: 8.71\n",
            "It: 13100, Loss: 8.485e-02, Loss_res: 3.425e-03,  Loss_bcs: 7.900e-02, Loss_ut_ics: 2.429e-03,, Time: 8.26\n",
            "It: 13200, Loss: 9.030e-02, Loss_res: 2.809e-03,  Loss_bcs: 8.491e-02, Loss_ut_ics: 2.584e-03,, Time: 8.91\n",
            "It: 13300, Loss: 8.562e-02, Loss_res: 4.811e-03,  Loss_bcs: 7.736e-02, Loss_ut_ics: 3.452e-03,, Time: 8.01\n",
            "It: 13400, Loss: 8.401e-02, Loss_res: 8.842e-03,  Loss_bcs: 7.231e-02, Loss_ut_ics: 2.860e-03,, Time: 9.24\n",
            "It: 13500, Loss: 7.867e-02, Loss_res: 7.415e-04,  Loss_bcs: 7.625e-02, Loss_ut_ics: 1.677e-03,, Time: 7.73\n",
            "It: 13600, Loss: 7.843e-02, Loss_res: 1.554e-03,  Loss_bcs: 7.531e-02, Loss_ut_ics: 1.561e-03,, Time: 9.59\n",
            "It: 13700, Loss: 8.741e-02, Loss_res: 9.095e-04,  Loss_bcs: 8.379e-02, Loss_ut_ics: 2.715e-03,, Time: 7.66\n",
            "It: 13800, Loss: 9.666e-02, Loss_res: 1.193e-02,  Loss_bcs: 8.182e-02, Loss_ut_ics: 2.910e-03,, Time: 9.91\n",
            "It: 13900, Loss: 8.382e-02, Loss_res: 1.949e-03,  Loss_bcs: 7.790e-02, Loss_ut_ics: 3.968e-03,, Time: 7.37\n",
            "It: 14000, Loss: 8.571e-02, Loss_res: 1.548e-03,  Loss_bcs: 8.120e-02, Loss_ut_ics: 2.962e-03,, Time: 9.68\n",
            "It: 14100, Loss: 8.557e-02, Loss_res: 4.949e-03,  Loss_bcs: 7.242e-02, Loss_ut_ics: 8.207e-03,, Time: 7.44\n",
            "It: 14200, Loss: 8.264e-02, Loss_res: 5.321e-03,  Loss_bcs: 7.440e-02, Loss_ut_ics: 2.921e-03,, Time: 9.69\n",
            "It: 14300, Loss: 8.596e-02, Loss_res: 2.292e-03,  Loss_bcs: 8.053e-02, Loss_ut_ics: 3.143e-03,, Time: 7.75\n",
            "It: 14400, Loss: 1.279e-01, Loss_res: 4.679e-02,  Loss_bcs: 7.811e-02, Loss_ut_ics: 3.045e-03,, Time: 9.68\n",
            "It: 14500, Loss: 8.450e-02, Loss_res: 1.971e-03,  Loss_bcs: 7.595e-02, Loss_ut_ics: 6.579e-03,, Time: 9.89\n",
            "It: 14600, Loss: 1.023e-01, Loss_res: 5.585e-03,  Loss_bcs: 9.306e-02, Loss_ut_ics: 3.623e-03,, Time: 9.02\n",
            "It: 14700, Loss: 7.493e-02, Loss_res: 4.775e-03,  Loss_bcs: 6.679e-02, Loss_ut_ics: 3.368e-03,, Time: 8.56\n",
            "It: 14800, Loss: 8.697e-02, Loss_res: 7.658e-03,  Loss_bcs: 7.568e-02, Loss_ut_ics: 3.631e-03,, Time: 8.82\n",
            "It: 14900, Loss: 7.797e-02, Loss_res: 1.631e-03,  Loss_bcs: 7.297e-02, Loss_ut_ics: 3.367e-03,, Time: 8.96\n",
            "It: 15000, Loss: 8.017e-02, Loss_res: 1.468e-03,  Loss_bcs: 7.563e-02, Loss_ut_ics: 3.067e-03,, Time: 8.00\n",
            "It: 15100, Loss: 7.395e-02, Loss_res: 2.278e-03,  Loss_bcs: 6.882e-02, Loss_ut_ics: 2.852e-03,, Time: 9.47\n",
            "It: 15200, Loss: 7.862e-02, Loss_res: 2.718e-03,  Loss_bcs: 6.774e-02, Loss_ut_ics: 8.164e-03,, Time: 7.63\n",
            "It: 15300, Loss: 9.750e-02, Loss_res: 2.181e-02,  Loss_bcs: 7.059e-02, Loss_ut_ics: 5.101e-03,, Time: 9.82\n",
            "It: 15400, Loss: 2.465e-01, Loss_res: 1.622e-01,  Loss_bcs: 7.046e-02, Loss_ut_ics: 1.383e-02,, Time: 7.42\n",
            "It: 15500, Loss: 8.026e-02, Loss_res: 3.847e-03,  Loss_bcs: 7.096e-02, Loss_ut_ics: 5.459e-03,, Time: 9.82\n",
            "It: 15600, Loss: 7.091e-02, Loss_res: 8.545e-04,  Loss_bcs: 6.731e-02, Loss_ut_ics: 2.744e-03,, Time: 7.24\n",
            "It: 15700, Loss: 8.356e-02, Loss_res: 3.045e-03,  Loss_bcs: 7.652e-02, Loss_ut_ics: 3.996e-03,, Time: 9.76\n",
            "It: 15800, Loss: 8.848e-02, Loss_res: 9.150e-03,  Loss_bcs: 7.225e-02, Loss_ut_ics: 7.086e-03,, Time: 7.38\n",
            "It: 15900, Loss: 6.432e-02, Loss_res: 1.201e-03,  Loss_bcs: 5.962e-02, Loss_ut_ics: 3.504e-03,, Time: 9.43\n",
            "It: 16000, Loss: 6.630e-02, Loss_res: 1.047e-03,  Loss_bcs: 6.163e-02, Loss_ut_ics: 3.625e-03,, Time: 7.63\n",
            "It: 16100, Loss: 8.027e-02, Loss_res: 1.294e-03,  Loss_bcs: 7.623e-02, Loss_ut_ics: 2.747e-03,, Time: 9.21\n",
            "It: 16200, Loss: 7.243e-02, Loss_res: 1.316e-03,  Loss_bcs: 6.807e-02, Loss_ut_ics: 3.040e-03,, Time: 7.82\n",
            "It: 16300, Loss: 7.468e-02, Loss_res: 1.304e-03,  Loss_bcs: 6.907e-02, Loss_ut_ics: 4.303e-03,, Time: 9.07\n",
            "It: 16400, Loss: 7.181e-02, Loss_res: 2.021e-03,  Loss_bcs: 6.599e-02, Loss_ut_ics: 3.792e-03,, Time: 8.06\n",
            "It: 16500, Loss: 7.945e-02, Loss_res: 1.439e-02,  Loss_bcs: 6.073e-02, Loss_ut_ics: 4.330e-03,, Time: 8.71\n",
            "It: 16600, Loss: 7.218e-02, Loss_res: 2.990e-03,  Loss_bcs: 6.528e-02, Loss_ut_ics: 3.907e-03,, Time: 8.36\n",
            "It: 16700, Loss: 7.971e-02, Loss_res: 1.758e-03,  Loss_bcs: 7.379e-02, Loss_ut_ics: 4.166e-03,, Time: 8.44\n",
            "It: 16800, Loss: 7.788e-02, Loss_res: 6.765e-03,  Loss_bcs: 6.703e-02, Loss_ut_ics: 4.080e-03,, Time: 8.58\n",
            "It: 16900, Loss: 7.578e-02, Loss_res: 4.354e-03,  Loss_bcs: 6.681e-02, Loss_ut_ics: 4.615e-03,, Time: 8.27\n",
            "It: 17000, Loss: 7.846e-02, Loss_res: 5.295e-03,  Loss_bcs: 7.009e-02, Loss_ut_ics: 3.076e-03,, Time: 9.04\n",
            "It: 17100, Loss: 8.223e-02, Loss_res: 6.151e-03,  Loss_bcs: 7.147e-02, Loss_ut_ics: 4.610e-03,, Time: 7.91\n",
            "It: 17200, Loss: 7.316e-02, Loss_res: 2.122e-03,  Loss_bcs: 6.601e-02, Loss_ut_ics: 5.029e-03,, Time: 9.58\n",
            "It: 17300, Loss: 1.237e-01, Loss_res: 5.317e-02,  Loss_bcs: 6.310e-02, Loss_ut_ics: 7.448e-03,, Time: 7.69\n",
            "It: 17400, Loss: 7.231e-02, Loss_res: 2.043e-03,  Loss_bcs: 6.718e-02, Loss_ut_ics: 3.086e-03,, Time: 9.94\n",
            "It: 17500, Loss: 7.217e-02, Loss_res: 3.057e-03,  Loss_bcs: 6.582e-02, Loss_ut_ics: 3.295e-03,, Time: 6.96\n",
            "It: 17600, Loss: 6.269e-02, Loss_res: 9.389e-04,  Loss_bcs: 5.795e-02, Loss_ut_ics: 3.803e-03,, Time: 10.12\n",
            "It: 17700, Loss: 8.809e-02, Loss_res: 2.677e-02,  Loss_bcs: 5.780e-02, Loss_ut_ics: 3.523e-03,, Time: 7.38\n",
            "It: 17800, Loss: 7.312e-02, Loss_res: 1.247e-03,  Loss_bcs: 6.842e-02, Loss_ut_ics: 3.458e-03,, Time: 9.74\n",
            "It: 17900, Loss: 7.950e-02, Loss_res: 1.450e-03,  Loss_bcs: 7.518e-02, Loss_ut_ics: 2.868e-03,, Time: 7.48\n",
            "It: 18000, Loss: 6.191e-02, Loss_res: 1.134e-03,  Loss_bcs: 5.779e-02, Loss_ut_ics: 2.984e-03,, Time: 9.55\n",
            "It: 18100, Loss: 8.167e-02, Loss_res: 3.368e-03,  Loss_bcs: 7.318e-02, Loss_ut_ics: 5.120e-03,, Time: 8.03\n",
            "It: 18200, Loss: 6.853e-02, Loss_res: 1.668e-03,  Loss_bcs: 6.336e-02, Loss_ut_ics: 3.498e-03,, Time: 9.28\n",
            "It: 18300, Loss: 6.730e-02, Loss_res: 1.580e-03,  Loss_bcs: 6.078e-02, Loss_ut_ics: 4.940e-03,, Time: 8.09\n",
            "It: 18400, Loss: 6.843e-02, Loss_res: 1.478e-03,  Loss_bcs: 6.218e-02, Loss_ut_ics: 4.771e-03,, Time: 8.91\n",
            "It: 18500, Loss: 8.719e-02, Loss_res: 2.343e-02,  Loss_bcs: 6.083e-02, Loss_ut_ics: 2.924e-03,, Time: 8.13\n",
            "It: 18600, Loss: 6.833e-02, Loss_res: 9.169e-04,  Loss_bcs: 6.345e-02, Loss_ut_ics: 3.962e-03,, Time: 8.50\n",
            "It: 18700, Loss: 6.591e-02, Loss_res: 1.112e-03,  Loss_bcs: 6.183e-02, Loss_ut_ics: 2.972e-03,, Time: 8.58\n",
            "It: 18800, Loss: 8.406e-02, Loss_res: 6.814e-03,  Loss_bcs: 7.093e-02, Loss_ut_ics: 6.318e-03,, Time: 8.10\n",
            "It: 18900, Loss: 7.264e-02, Loss_res: 6.531e-03,  Loss_bcs: 6.110e-02, Loss_ut_ics: 5.011e-03,, Time: 8.94\n",
            "It: 19000, Loss: 6.973e-02, Loss_res: 1.140e-03,  Loss_bcs: 6.542e-02, Loss_ut_ics: 3.174e-03,, Time: 7.82\n",
            "It: 19100, Loss: 6.634e-02, Loss_res: 1.790e-03,  Loss_bcs: 5.992e-02, Loss_ut_ics: 4.627e-03,, Time: 9.28\n",
            "It: 19200, Loss: 7.316e-02, Loss_res: 5.845e-03,  Loss_bcs: 6.360e-02, Loss_ut_ics: 3.715e-03,, Time: 7.50\n",
            "It: 19300, Loss: 6.799e-02, Loss_res: 2.488e-03,  Loss_bcs: 6.146e-02, Loss_ut_ics: 4.040e-03,, Time: 9.68\n",
            "It: 19400, Loss: 6.662e-02, Loss_res: 1.935e-03,  Loss_bcs: 6.206e-02, Loss_ut_ics: 2.624e-03,, Time: 7.05\n",
            "It: 19500, Loss: 1.142e-01, Loss_res: 4.365e-02,  Loss_bcs: 5.941e-02, Loss_ut_ics: 1.112e-02,, Time: 9.83\n",
            "It: 19600, Loss: 7.000e-02, Loss_res: 1.158e-03,  Loss_bcs: 6.530e-02, Loss_ut_ics: 3.534e-03,, Time: 7.14\n",
            "It: 19700, Loss: 7.068e-02, Loss_res: 2.090e-03,  Loss_bcs: 6.427e-02, Loss_ut_ics: 4.322e-03,, Time: 9.76\n",
            "It: 19800, Loss: 7.107e-02, Loss_res: 3.671e-03,  Loss_bcs: 6.432e-02, Loss_ut_ics: 3.081e-03,, Time: 7.41\n",
            "It: 19900, Loss: 7.706e-02, Loss_res: 5.673e-03,  Loss_bcs: 6.729e-02, Loss_ut_ics: 4.094e-03,, Time: 10.72\n",
            "It: 20000, Loss: 8.280e-02, Loss_res: 9.655e-03,  Loss_bcs: 6.973e-02, Loss_ut_ics: 3.411e-03,, Time: 7.65\n",
            "It: 20100, Loss: 1.118e-01, Loss_res: 3.312e-02,  Loss_bcs: 7.344e-02, Loss_ut_ics: 5.218e-03,, Time: 9.14\n",
            "It: 20200, Loss: 6.576e-02, Loss_res: 1.352e-03,  Loss_bcs: 6.068e-02, Loss_ut_ics: 3.719e-03,, Time: 8.09\n",
            "It: 20300, Loss: 6.278e-02, Loss_res: 5.871e-04,  Loss_bcs: 5.852e-02, Loss_ut_ics: 3.675e-03,, Time: 8.85\n",
            "It: 20400, Loss: 6.444e-02, Loss_res: 2.215e-03,  Loss_bcs: 5.850e-02, Loss_ut_ics: 3.727e-03,, Time: 8.25\n",
            "It: 20500, Loss: 7.216e-02, Loss_res: 1.557e-03,  Loss_bcs: 6.636e-02, Loss_ut_ics: 4.245e-03,, Time: 8.70\n",
            "It: 20600, Loss: 7.252e-02, Loss_res: 6.790e-04,  Loss_bcs: 6.614e-02, Loss_ut_ics: 5.707e-03,, Time: 8.56\n",
            "It: 20700, Loss: 7.031e-02, Loss_res: 5.594e-03,  Loss_bcs: 6.064e-02, Loss_ut_ics: 4.071e-03,, Time: 8.13\n",
            "It: 20800, Loss: 6.516e-02, Loss_res: 8.716e-03,  Loss_bcs: 5.394e-02, Loss_ut_ics: 2.506e-03,, Time: 8.94\n",
            "It: 20900, Loss: 6.439e-02, Loss_res: 1.709e-03,  Loss_bcs: 5.903e-02, Loss_ut_ics: 3.646e-03,, Time: 7.86\n",
            "It: 21000, Loss: 6.555e-02, Loss_res: 1.223e-03,  Loss_bcs: 5.988e-02, Loss_ut_ics: 4.448e-03,, Time: 9.39\n",
            "It: 21100, Loss: 6.960e-02, Loss_res: 8.460e-04,  Loss_bcs: 6.509e-02, Loss_ut_ics: 3.667e-03,, Time: 7.49\n",
            "It: 21200, Loss: 5.873e-02, Loss_res: 8.136e-04,  Loss_bcs: 5.490e-02, Loss_ut_ics: 3.013e-03,, Time: 9.75\n",
            "It: 21300, Loss: 7.079e-02, Loss_res: 3.278e-03,  Loss_bcs: 6.400e-02, Loss_ut_ics: 3.507e-03,, Time: 7.06\n",
            "It: 21400, Loss: 8.720e-02, Loss_res: 1.326e-02,  Loss_bcs: 6.360e-02, Loss_ut_ics: 1.034e-02,, Time: 9.76\n",
            "It: 21500, Loss: 6.305e-02, Loss_res: 1.493e-03,  Loss_bcs: 5.819e-02, Loss_ut_ics: 3.372e-03,, Time: 7.37\n",
            "It: 21600, Loss: 6.000e-02, Loss_res: 1.001e-03,  Loss_bcs: 5.630e-02, Loss_ut_ics: 2.703e-03,, Time: 9.35\n",
            "It: 21700, Loss: 6.630e-02, Loss_res: 1.163e-03,  Loss_bcs: 6.200e-02, Loss_ut_ics: 3.140e-03,, Time: 7.40\n",
            "It: 21800, Loss: 6.741e-02, Loss_res: 1.123e-03,  Loss_bcs: 6.335e-02, Loss_ut_ics: 2.932e-03,, Time: 9.53\n",
            "It: 21900, Loss: 6.171e-02, Loss_res: 9.992e-04,  Loss_bcs: 5.714e-02, Loss_ut_ics: 3.579e-03,, Time: 7.59\n",
            "It: 22000, Loss: 5.661e-02, Loss_res: 1.222e-03,  Loss_bcs: 5.247e-02, Loss_ut_ics: 2.915e-03,, Time: 9.14\n",
            "It: 22100, Loss: 7.667e-02, Loss_res: 1.048e-02,  Loss_bcs: 6.150e-02, Loss_ut_ics: 4.684e-03,, Time: 7.75\n",
            "It: 22200, Loss: 6.686e-02, Loss_res: 2.361e-03,  Loss_bcs: 6.106e-02, Loss_ut_ics: 3.442e-03,, Time: 9.15\n",
            "It: 22300, Loss: 6.778e-02, Loss_res: 2.480e-03,  Loss_bcs: 6.077e-02, Loss_ut_ics: 4.529e-03,, Time: 8.21\n",
            "It: 22400, Loss: 6.289e-02, Loss_res: 8.828e-04,  Loss_bcs: 5.820e-02, Loss_ut_ics: 3.813e-03,, Time: 8.73\n",
            "It: 22500, Loss: 5.992e-02, Loss_res: 7.411e-04,  Loss_bcs: 5.634e-02, Loss_ut_ics: 2.843e-03,, Time: 8.47\n",
            "It: 22600, Loss: 2.287e-01, Loss_res: 1.522e-01,  Loss_bcs: 6.161e-02, Loss_ut_ics: 1.489e-02,, Time: 8.58\n",
            "It: 22700, Loss: 7.275e-02, Loss_res: 4.916e-03,  Loss_bcs: 6.318e-02, Loss_ut_ics: 4.649e-03,, Time: 8.78\n",
            "It: 22800, Loss: 7.262e-02, Loss_res: 7.867e-04,  Loss_bcs: 6.871e-02, Loss_ut_ics: 3.126e-03,, Time: 8.24\n",
            "It: 22900, Loss: 6.530e-02, Loss_res: 2.276e-03,  Loss_bcs: 5.970e-02, Loss_ut_ics: 3.329e-03,, Time: 8.86\n",
            "It: 23000, Loss: 6.924e-02, Loss_res: 1.827e-03,  Loss_bcs: 6.473e-02, Loss_ut_ics: 2.684e-03,, Time: 7.87\n",
            "It: 23100, Loss: 7.534e-02, Loss_res: 6.113e-04,  Loss_bcs: 7.109e-02, Loss_ut_ics: 3.634e-03,, Time: 9.72\n",
            "It: 23200, Loss: 7.450e-02, Loss_res: 3.794e-03,  Loss_bcs: 6.626e-02, Loss_ut_ics: 4.445e-03,, Time: 7.43\n",
            "It: 23300, Loss: 6.292e-02, Loss_res: 8.383e-04,  Loss_bcs: 5.932e-02, Loss_ut_ics: 2.760e-03,, Time: 9.95\n",
            "It: 23400, Loss: 6.237e-02, Loss_res: 6.643e-04,  Loss_bcs: 5.880e-02, Loss_ut_ics: 2.904e-03,, Time: 7.11\n",
            "It: 23500, Loss: 6.685e-02, Loss_res: 8.021e-04,  Loss_bcs: 6.207e-02, Loss_ut_ics: 3.981e-03,, Time: 9.76\n",
            "It: 23600, Loss: 6.327e-02, Loss_res: 1.063e-03,  Loss_bcs: 5.911e-02, Loss_ut_ics: 3.101e-03,, Time: 7.13\n",
            "It: 23700, Loss: 6.510e-02, Loss_res: 2.504e-03,  Loss_bcs: 5.931e-02, Loss_ut_ics: 3.286e-03,, Time: 9.58\n",
            "It: 23800, Loss: 6.634e-02, Loss_res: 1.606e-03,  Loss_bcs: 6.048e-02, Loss_ut_ics: 4.254e-03,, Time: 7.29\n",
            "It: 23900, Loss: 6.577e-02, Loss_res: 1.632e-03,  Loss_bcs: 6.054e-02, Loss_ut_ics: 3.594e-03,, Time: 9.26\n",
            "It: 24000, Loss: 6.765e-02, Loss_res: 6.835e-04,  Loss_bcs: 6.365e-02, Loss_ut_ics: 3.317e-03,, Time: 7.43\n",
            "It: 24100, Loss: 5.678e-02, Loss_res: 2.980e-03,  Loss_bcs: 5.074e-02, Loss_ut_ics: 3.068e-03,, Time: 9.40\n",
            "It: 24200, Loss: 7.678e-02, Loss_res: 8.510e-03,  Loss_bcs: 6.254e-02, Loss_ut_ics: 5.728e-03,, Time: 7.81\n",
            "It: 24300, Loss: 6.063e-02, Loss_res: 1.526e-03,  Loss_bcs: 5.534e-02, Loss_ut_ics: 3.767e-03,, Time: 9.07\n",
            "It: 24400, Loss: 5.878e-02, Loss_res: 2.348e-03,  Loss_bcs: 5.276e-02, Loss_ut_ics: 3.667e-03,, Time: 7.79\n",
            "It: 24500, Loss: 6.025e-02, Loss_res: 7.872e-04,  Loss_bcs: 5.659e-02, Loss_ut_ics: 2.878e-03,, Time: 9.04\n",
            "It: 24600, Loss: 6.184e-02, Loss_res: 1.312e-03,  Loss_bcs: 5.643e-02, Loss_ut_ics: 4.094e-03,, Time: 8.33\n",
            "It: 24700, Loss: 5.781e-02, Loss_res: 2.640e-03,  Loss_bcs: 5.257e-02, Loss_ut_ics: 2.603e-03,, Time: 8.88\n",
            "It: 24800, Loss: 7.212e-02, Loss_res: 6.286e-03,  Loss_bcs: 6.193e-02, Loss_ut_ics: 3.905e-03,, Time: 8.22\n",
            "It: 24900, Loss: 4.881e-02, Loss_res: 1.559e-03,  Loss_bcs: 4.353e-02, Loss_ut_ics: 3.718e-03,, Time: 8.67\n",
            "It: 25000, Loss: 6.189e-02, Loss_res: 1.097e-03,  Loss_bcs: 5.727e-02, Loss_ut_ics: 3.520e-03,, Time: 8.60\n",
            "It: 25100, Loss: 5.449e-02, Loss_res: 1.149e-03,  Loss_bcs: 4.987e-02, Loss_ut_ics: 3.474e-03,, Time: 8.38\n",
            "It: 25200, Loss: 6.163e-02, Loss_res: 1.655e-03,  Loss_bcs: 5.670e-02, Loss_ut_ics: 3.273e-03,, Time: 9.23\n",
            "It: 25300, Loss: 5.905e-02, Loss_res: 1.660e-03,  Loss_bcs: 5.392e-02, Loss_ut_ics: 3.466e-03,, Time: 8.45\n",
            "It: 25400, Loss: 5.554e-02, Loss_res: 6.916e-04,  Loss_bcs: 5.172e-02, Loss_ut_ics: 3.132e-03,, Time: 9.54\n",
            "It: 25500, Loss: 6.779e-02, Loss_res: 1.912e-03,  Loss_bcs: 6.234e-02, Loss_ut_ics: 3.534e-03,, Time: 7.38\n",
            "It: 25600, Loss: 6.569e-02, Loss_res: 8.998e-04,  Loss_bcs: 6.222e-02, Loss_ut_ics: 2.572e-03,, Time: 10.16\n",
            "It: 25700, Loss: 6.024e-02, Loss_res: 1.043e-03,  Loss_bcs: 5.609e-02, Loss_ut_ics: 3.111e-03,, Time: 7.38\n",
            "It: 25800, Loss: 7.498e-02, Loss_res: 4.413e-03,  Loss_bcs: 6.696e-02, Loss_ut_ics: 3.606e-03,, Time: 10.23\n",
            "It: 25900, Loss: 6.453e-02, Loss_res: 9.612e-04,  Loss_bcs: 6.015e-02, Loss_ut_ics: 3.424e-03,, Time: 7.21\n",
            "It: 26000, Loss: 5.775e-02, Loss_res: 2.061e-03,  Loss_bcs: 5.171e-02, Loss_ut_ics: 3.976e-03,, Time: 9.32\n",
            "It: 26100, Loss: 5.367e-02, Loss_res: 2.251e-03,  Loss_bcs: 4.734e-02, Loss_ut_ics: 4.082e-03,, Time: 7.27\n",
            "It: 26200, Loss: 8.004e-02, Loss_res: 1.174e-02,  Loss_bcs: 6.212e-02, Loss_ut_ics: 6.173e-03,, Time: 9.02\n",
            "It: 26300, Loss: 6.014e-02, Loss_res: 1.151e-03,  Loss_bcs: 5.642e-02, Loss_ut_ics: 2.573e-03,, Time: 7.45\n",
            "It: 26400, Loss: 5.949e-02, Loss_res: 5.686e-04,  Loss_bcs: 5.595e-02, Loss_ut_ics: 2.970e-03,, Time: 9.33\n",
            "It: 26500, Loss: 5.731e-02, Loss_res: 9.557e-04,  Loss_bcs: 5.304e-02, Loss_ut_ics: 3.317e-03,, Time: 7.25\n",
            "It: 26600, Loss: 6.558e-02, Loss_res: 5.749e-04,  Loss_bcs: 6.223e-02, Loss_ut_ics: 2.777e-03,, Time: 9.13\n",
            "It: 26700, Loss: 7.143e-02, Loss_res: 1.435e-03,  Loss_bcs: 6.700e-02, Loss_ut_ics: 2.992e-03,, Time: 7.52\n",
            "It: 26800, Loss: 6.661e-02, Loss_res: 5.430e-03,  Loss_bcs: 5.799e-02, Loss_ut_ics: 3.185e-03,, Time: 9.05\n",
            "It: 26900, Loss: 5.712e-02, Loss_res: 1.427e-03,  Loss_bcs: 5.195e-02, Loss_ut_ics: 3.745e-03,, Time: 7.86\n",
            "It: 27000, Loss: 6.415e-02, Loss_res: 1.044e-03,  Loss_bcs: 6.034e-02, Loss_ut_ics: 2.771e-03,, Time: 8.69\n",
            "It: 27100, Loss: 5.290e-02, Loss_res: 1.223e-03,  Loss_bcs: 4.876e-02, Loss_ut_ics: 2.917e-03,, Time: 7.83\n",
            "It: 27200, Loss: 5.995e-02, Loss_res: 9.051e-04,  Loss_bcs: 5.579e-02, Loss_ut_ics: 3.259e-03,, Time: 8.96\n",
            "It: 27300, Loss: 6.308e-02, Loss_res: 1.959e-03,  Loss_bcs: 5.731e-02, Loss_ut_ics: 3.810e-03,, Time: 8.37\n",
            "It: 27400, Loss: 7.381e-02, Loss_res: 2.281e-03,  Loss_bcs: 6.835e-02, Loss_ut_ics: 3.179e-03,, Time: 9.13\n",
            "It: 27500, Loss: 6.470e-02, Loss_res: 9.392e-04,  Loss_bcs: 6.097e-02, Loss_ut_ics: 2.791e-03,, Time: 8.70\n",
            "It: 27600, Loss: 7.353e-02, Loss_res: 3.118e-03,  Loss_bcs: 6.732e-02, Loss_ut_ics: 3.087e-03,, Time: 9.12\n",
            "It: 27700, Loss: 5.560e-02, Loss_res: 3.023e-03,  Loss_bcs: 5.003e-02, Loss_ut_ics: 2.546e-03,, Time: 8.95\n",
            "It: 27800, Loss: 5.693e-02, Loss_res: 9.729e-04,  Loss_bcs: 5.347e-02, Loss_ut_ics: 2.487e-03,, Time: 9.22\n",
            "It: 27900, Loss: 6.218e-02, Loss_res: 2.529e-03,  Loss_bcs: 5.649e-02, Loss_ut_ics: 3.159e-03,, Time: 15.32\n",
            "It: 28000, Loss: 6.663e-02, Loss_res: 1.010e-03,  Loss_bcs: 6.229e-02, Loss_ut_ics: 3.322e-03,, Time: 14.74\n",
            "It: 28100, Loss: 5.748e-02, Loss_res: 1.653e-03,  Loss_bcs: 5.343e-02, Loss_ut_ics: 2.399e-03,, Time: 18.65\n",
            "It: 28200, Loss: 5.982e-02, Loss_res: 8.396e-04,  Loss_bcs: 5.617e-02, Loss_ut_ics: 2.807e-03,, Time: 14.89\n",
            "It: 28300, Loss: 6.129e-02, Loss_res: 2.010e-03,  Loss_bcs: 5.704e-02, Loss_ut_ics: 2.244e-03,, Time: 18.47\n",
            "It: 28400, Loss: 5.918e-02, Loss_res: 3.378e-03,  Loss_bcs: 5.304e-02, Loss_ut_ics: 2.756e-03,, Time: 15.55\n",
            "It: 28500, Loss: 5.875e-02, Loss_res: 7.870e-04,  Loss_bcs: 5.567e-02, Loss_ut_ics: 2.290e-03,, Time: 15.15\n",
            "It: 28600, Loss: 5.903e-02, Loss_res: 1.548e-03,  Loss_bcs: 5.457e-02, Loss_ut_ics: 2.912e-03,, Time: 8.79\n",
            "It: 28700, Loss: 5.713e-02, Loss_res: 1.163e-03,  Loss_bcs: 5.360e-02, Loss_ut_ics: 2.371e-03,, Time: 11.37\n",
            "It: 28800, Loss: 5.787e-02, Loss_res: 1.490e-03,  Loss_bcs: 5.322e-02, Loss_ut_ics: 3.159e-03,, Time: 17.01\n",
            "It: 28900, Loss: 6.003e-02, Loss_res: 1.933e-03,  Loss_bcs: 5.522e-02, Loss_ut_ics: 2.883e-03,, Time: 15.55\n",
            "It: 29000, Loss: 5.380e-02, Loss_res: 1.815e-03,  Loss_bcs: 4.992e-02, Loss_ut_ics: 2.067e-03,, Time: 18.10\n",
            "It: 29100, Loss: 6.252e-02, Loss_res: 1.052e-03,  Loss_bcs: 5.893e-02, Loss_ut_ics: 2.539e-03,, Time: 14.30\n",
            "It: 29200, Loss: 5.907e-02, Loss_res: 1.492e-03,  Loss_bcs: 5.463e-02, Loss_ut_ics: 2.945e-03,, Time: 18.29\n",
            "It: 29300, Loss: 6.224e-02, Loss_res: 2.180e-03,  Loss_bcs: 5.692e-02, Loss_ut_ics: 3.144e-03,, Time: 15.10\n",
            "It: 29400, Loss: 5.538e-02, Loss_res: 9.342e-04,  Loss_bcs: 5.228e-02, Loss_ut_ics: 2.159e-03,, Time: 17.58\n",
            "It: 29500, Loss: 5.492e-02, Loss_res: 7.900e-04,  Loss_bcs: 5.152e-02, Loss_ut_ics: 2.608e-03,, Time: 15.74\n",
            "It: 29600, Loss: 6.064e-02, Loss_res: 6.278e-04,  Loss_bcs: 5.735e-02, Loss_ut_ics: 2.658e-03,, Time: 17.20\n",
            "It: 29700, Loss: 6.871e-02, Loss_res: 1.595e-03,  Loss_bcs: 6.395e-02, Loss_ut_ics: 3.163e-03,, Time: 16.59\n",
            "It: 29800, Loss: 5.005e-02, Loss_res: 1.119e-03,  Loss_bcs: 4.643e-02, Loss_ut_ics: 2.508e-03,, Time: 15.57\n",
            "It: 29900, Loss: 5.814e-02, Loss_res: 5.049e-04,  Loss_bcs: 5.520e-02, Loss_ut_ics: 2.428e-03,, Time: 17.91\n",
            "It: 30000, Loss: 5.313e-02, Loss_res: 6.308e-04,  Loss_bcs: 5.029e-02, Loss_ut_ics: 2.209e-03,, Time: 14.41\n",
            "It: 30100, Loss: 4.795e-02, Loss_res: 7.329e-04,  Loss_bcs: 4.504e-02, Loss_ut_ics: 2.177e-03,, Time: 18.21\n",
            "It: 30200, Loss: 6.440e-02, Loss_res: 3.765e-03,  Loss_bcs: 5.811e-02, Loss_ut_ics: 2.526e-03,, Time: 15.00\n",
            "It: 30300, Loss: 6.378e-02, Loss_res: 1.072e-02,  Loss_bcs: 5.050e-02, Loss_ut_ics: 2.561e-03,, Time: 21.50\n",
            "It: 30400, Loss: 5.872e-02, Loss_res: 1.393e-03,  Loss_bcs: 5.402e-02, Loss_ut_ics: 3.313e-03,, Time: 21.36\n",
            "It: 30500, Loss: 5.434e-02, Loss_res: 1.954e-03,  Loss_bcs: 4.983e-02, Loss_ut_ics: 2.551e-03,, Time: 15.11\n",
            "It: 30600, Loss: 6.626e-02, Loss_res: 9.696e-04,  Loss_bcs: 6.208e-02, Loss_ut_ics: 3.208e-03,, Time: 19.34\n",
            "It: 30700, Loss: 5.811e-02, Loss_res: 1.067e-03,  Loss_bcs: 5.509e-02, Loss_ut_ics: 1.958e-03,, Time: 16.94\n",
            "It: 30800, Loss: 5.437e-02, Loss_res: 9.975e-04,  Loss_bcs: 5.107e-02, Loss_ut_ics: 2.303e-03,, Time: 18.49\n",
            "It: 30900, Loss: 5.154e-02, Loss_res: 1.213e-03,  Loss_bcs: 4.842e-02, Loss_ut_ics: 1.906e-03,, Time: 17.32\n",
            "It: 31000, Loss: 5.362e-02, Loss_res: 1.225e-03,  Loss_bcs: 5.015e-02, Loss_ut_ics: 2.245e-03,, Time: 17.40\n",
            "It: 31100, Loss: 5.315e-02, Loss_res: 7.831e-04,  Loss_bcs: 4.951e-02, Loss_ut_ics: 2.854e-03,, Time: 18.95\n",
            "It: 31200, Loss: 5.477e-02, Loss_res: 1.364e-03,  Loss_bcs: 5.128e-02, Loss_ut_ics: 2.124e-03,, Time: 15.26\n",
            "It: 31300, Loss: 6.323e-02, Loss_res: 5.696e-03,  Loss_bcs: 5.522e-02, Loss_ut_ics: 2.306e-03,, Time: 20.51\n",
            "It: 31400, Loss: 5.627e-02, Loss_res: 7.567e-03,  Loss_bcs: 4.628e-02, Loss_ut_ics: 2.416e-03,, Time: 16.82\n",
            "It: 31500, Loss: 6.262e-02, Loss_res: 1.672e-03,  Loss_bcs: 5.764e-02, Loss_ut_ics: 3.306e-03,, Time: 18.37\n",
            "It: 31600, Loss: 6.000e-02, Loss_res: 8.037e-04,  Loss_bcs: 5.643e-02, Loss_ut_ics: 2.765e-03,, Time: 18.68\n",
            "It: 31700, Loss: 6.137e-02, Loss_res: 1.572e-03,  Loss_bcs: 5.740e-02, Loss_ut_ics: 2.401e-03,, Time: 16.93\n",
            "It: 31800, Loss: 5.750e-02, Loss_res: 2.474e-03,  Loss_bcs: 5.245e-02, Loss_ut_ics: 2.580e-03,, Time: 18.95\n",
            "It: 31900, Loss: 5.533e-02, Loss_res: 8.117e-04,  Loss_bcs: 5.267e-02, Loss_ut_ics: 1.849e-03,, Time: 14.38\n",
            "It: 32000, Loss: 5.089e-02, Loss_res: 1.773e-03,  Loss_bcs: 4.686e-02, Loss_ut_ics: 2.252e-03,, Time: 18.96\n",
            "It: 32100, Loss: 5.320e-02, Loss_res: 2.041e-03,  Loss_bcs: 4.890e-02, Loss_ut_ics: 2.257e-03,, Time: 15.84\n",
            "It: 32200, Loss: 5.844e-02, Loss_res: 1.619e-03,  Loss_bcs: 5.524e-02, Loss_ut_ics: 1.575e-03,, Time: 17.28\n",
            "It: 32300, Loss: 5.342e-02, Loss_res: 1.141e-03,  Loss_bcs: 4.992e-02, Loss_ut_ics: 2.358e-03,, Time: 16.17\n",
            "It: 32400, Loss: 7.005e-02, Loss_res: 4.876e-03,  Loss_bcs: 6.281e-02, Loss_ut_ics: 2.364e-03,, Time: 16.24\n",
            "It: 32500, Loss: 5.832e-02, Loss_res: 1.089e-03,  Loss_bcs: 5.438e-02, Loss_ut_ics: 2.846e-03,, Time: 18.76\n",
            "It: 32600, Loss: 5.703e-02, Loss_res: 5.821e-04,  Loss_bcs: 5.380e-02, Loss_ut_ics: 2.655e-03,, Time: 14.74\n",
            "It: 32700, Loss: 6.356e-02, Loss_res: 2.059e-03,  Loss_bcs: 5.868e-02, Loss_ut_ics: 2.825e-03,, Time: 18.58\n",
            "It: 32800, Loss: 5.783e-02, Loss_res: 9.461e-04,  Loss_bcs: 5.499e-02, Loss_ut_ics: 1.895e-03,, Time: 15.38\n",
            "It: 32900, Loss: 5.897e-02, Loss_res: 1.103e-03,  Loss_bcs: 5.593e-02, Loss_ut_ics: 1.943e-03,, Time: 18.83\n",
            "It: 33000, Loss: 5.132e-02, Loss_res: 3.037e-03,  Loss_bcs: 4.623e-02, Loss_ut_ics: 2.052e-03,, Time: 18.75\n",
            "It: 33100, Loss: 5.783e-02, Loss_res: 3.288e-03,  Loss_bcs: 5.242e-02, Loss_ut_ics: 2.123e-03,, Time: 15.69\n",
            "It: 33200, Loss: 5.829e-02, Loss_res: 7.418e-04,  Loss_bcs: 5.532e-02, Loss_ut_ics: 2.232e-03,, Time: 18.97\n",
            "It: 33300, Loss: 5.660e-02, Loss_res: 1.351e-03,  Loss_bcs: 5.262e-02, Loss_ut_ics: 2.630e-03,, Time: 15.34\n",
            "It: 33400, Loss: 5.454e-02, Loss_res: 1.435e-03,  Loss_bcs: 5.081e-02, Loss_ut_ics: 2.300e-03,, Time: 18.06\n",
            "It: 33500, Loss: 5.008e-02, Loss_res: 1.061e-03,  Loss_bcs: 4.712e-02, Loss_ut_ics: 1.905e-03,, Time: 16.54\n",
            "It: 33600, Loss: 5.675e-02, Loss_res: 1.072e-03,  Loss_bcs: 5.346e-02, Loss_ut_ics: 2.211e-03,, Time: 17.17\n",
            "It: 33700, Loss: 4.869e-02, Loss_res: 4.543e-04,  Loss_bcs: 4.636e-02, Loss_ut_ics: 1.875e-03,, Time: 17.56\n",
            "It: 33800, Loss: 6.022e-02, Loss_res: 5.633e-03,  Loss_bcs: 5.191e-02, Loss_ut_ics: 2.672e-03,, Time: 16.09\n",
            "It: 33900, Loss: 5.300e-02, Loss_res: 6.145e-04,  Loss_bcs: 5.016e-02, Loss_ut_ics: 2.225e-03,, Time: 18.57\n",
            "It: 34000, Loss: 5.520e-02, Loss_res: 5.599e-04,  Loss_bcs: 5.254e-02, Loss_ut_ics: 2.102e-03,, Time: 14.75\n",
            "It: 34100, Loss: 5.954e-02, Loss_res: 5.953e-04,  Loss_bcs: 5.637e-02, Loss_ut_ics: 2.583e-03,, Time: 18.89\n",
            "It: 34200, Loss: 4.835e-02, Loss_res: 9.263e-04,  Loss_bcs: 4.549e-02, Loss_ut_ics: 1.941e-03,, Time: 15.08\n",
            "It: 34300, Loss: 5.401e-02, Loss_res: 1.026e-03,  Loss_bcs: 5.060e-02, Loss_ut_ics: 2.377e-03,, Time: 18.05\n",
            "It: 34400, Loss: 5.694e-02, Loss_res: 1.107e-03,  Loss_bcs: 5.377e-02, Loss_ut_ics: 2.064e-03,, Time: 16.17\n",
            "It: 34500, Loss: 6.701e-02, Loss_res: 8.330e-04,  Loss_bcs: 6.374e-02, Loss_ut_ics: 2.443e-03,, Time: 17.23\n",
            "It: 34600, Loss: 4.776e-02, Loss_res: 2.502e-03,  Loss_bcs: 4.263e-02, Loss_ut_ics: 2.623e-03,, Time: 17.33\n",
            "It: 34700, Loss: 5.228e-02, Loss_res: 1.039e-03,  Loss_bcs: 4.908e-02, Loss_ut_ics: 2.161e-03,, Time: 16.19\n",
            "It: 34800, Loss: 6.023e-02, Loss_res: 8.591e-04,  Loss_bcs: 5.730e-02, Loss_ut_ics: 2.075e-03,, Time: 18.56\n",
            "It: 34900, Loss: 5.311e-02, Loss_res: 1.872e-03,  Loss_bcs: 4.913e-02, Loss_ut_ics: 2.105e-03,, Time: 14.81\n",
            "It: 35000, Loss: 5.109e-02, Loss_res: 1.033e-03,  Loss_bcs: 4.751e-02, Loss_ut_ics: 2.547e-03,, Time: 18.67\n",
            "It: 35100, Loss: 5.268e-02, Loss_res: 6.918e-04,  Loss_bcs: 4.998e-02, Loss_ut_ics: 2.009e-03,, Time: 15.16\n",
            "It: 35200, Loss: 5.420e-02, Loss_res: 1.324e-03,  Loss_bcs: 5.095e-02, Loss_ut_ics: 1.926e-03,, Time: 17.84\n",
            "It: 35300, Loss: 5.673e-02, Loss_res: 1.068e-03,  Loss_bcs: 5.312e-02, Loss_ut_ics: 2.542e-03,, Time: 15.97\n",
            "It: 35400, Loss: 4.997e-02, Loss_res: 9.742e-04,  Loss_bcs: 4.732e-02, Loss_ut_ics: 1.668e-03,, Time: 18.19\n",
            "It: 35500, Loss: 5.483e-02, Loss_res: 1.253e-03,  Loss_bcs: 5.031e-02, Loss_ut_ics: 3.268e-03,, Time: 18.39\n",
            "It: 35600, Loss: 5.791e-02, Loss_res: 7.941e-04,  Loss_bcs: 5.490e-02, Loss_ut_ics: 2.210e-03,, Time: 17.28\n",
            "It: 35700, Loss: 6.246e-02, Loss_res: 4.112e-03,  Loss_bcs: 5.593e-02, Loss_ut_ics: 2.418e-03,, Time: 18.60\n",
            "It: 35800, Loss: 5.005e-02, Loss_res: 1.142e-03,  Loss_bcs: 4.687e-02, Loss_ut_ics: 2.041e-03,, Time: 14.58\n",
            "It: 35900, Loss: 5.090e-02, Loss_res: 8.110e-04,  Loss_bcs: 4.815e-02, Loss_ut_ics: 1.939e-03,, Time: 18.82\n",
            "It: 36000, Loss: 5.329e-02, Loss_res: 8.601e-04,  Loss_bcs: 5.005e-02, Loss_ut_ics: 2.378e-03,, Time: 15.63\n",
            "It: 36100, Loss: 4.946e-02, Loss_res: 6.980e-04,  Loss_bcs: 4.725e-02, Loss_ut_ics: 1.515e-03,, Time: 17.62\n",
            "It: 36200, Loss: 5.684e-02, Loss_res: 1.389e-03,  Loss_bcs: 5.320e-02, Loss_ut_ics: 2.249e-03,, Time: 17.06\n",
            "It: 36300, Loss: 4.581e-02, Loss_res: 1.692e-03,  Loss_bcs: 4.288e-02, Loss_ut_ics: 1.244e-03,, Time: 16.34\n",
            "It: 36400, Loss: 5.272e-02, Loss_res: 8.934e-04,  Loss_bcs: 4.931e-02, Loss_ut_ics: 2.517e-03,, Time: 18.06\n",
            "It: 36500, Loss: 5.525e-02, Loss_res: 8.114e-04,  Loss_bcs: 5.214e-02, Loss_ut_ics: 2.300e-03,, Time: 15.39\n",
            "It: 36600, Loss: 4.833e-02, Loss_res: 6.812e-04,  Loss_bcs: 4.540e-02, Loss_ut_ics: 2.245e-03,, Time: 21.07\n",
            "It: 36700, Loss: 4.979e-02, Loss_res: 6.902e-04,  Loss_bcs: 4.698e-02, Loss_ut_ics: 2.111e-03,, Time: 15.94\n",
            "It: 36800, Loss: 5.028e-02, Loss_res: 7.438e-04,  Loss_bcs: 4.735e-02, Loss_ut_ics: 2.187e-03,, Time: 17.56\n",
            "It: 36900, Loss: 5.811e-02, Loss_res: 9.521e-04,  Loss_bcs: 5.505e-02, Loss_ut_ics: 2.112e-03,, Time: 17.16\n",
            "It: 37000, Loss: 5.573e-02, Loss_res: 8.803e-04,  Loss_bcs: 5.219e-02, Loss_ut_ics: 2.664e-03,, Time: 16.39\n",
            "It: 37100, Loss: 5.073e-02, Loss_res: 9.199e-04,  Loss_bcs: 4.797e-02, Loss_ut_ics: 1.837e-03,, Time: 18.72\n",
            "It: 37200, Loss: 5.955e-02, Loss_res: 4.007e-03,  Loss_bcs: 5.311e-02, Loss_ut_ics: 2.427e-03,, Time: 14.85\n",
            "It: 37300, Loss: 5.507e-02, Loss_res: 8.342e-04,  Loss_bcs: 5.247e-02, Loss_ut_ics: 1.772e-03,, Time: 20.03\n",
            "It: 37400, Loss: 4.928e-02, Loss_res: 1.269e-03,  Loss_bcs: 4.656e-02, Loss_ut_ics: 1.453e-03,, Time: 15.89\n",
            "It: 37500, Loss: 5.144e-02, Loss_res: 7.736e-04,  Loss_bcs: 4.882e-02, Loss_ut_ics: 1.849e-03,, Time: 17.62\n",
            "It: 37600, Loss: 5.071e-02, Loss_res: 9.297e-04,  Loss_bcs: 4.805e-02, Loss_ut_ics: 1.727e-03,, Time: 17.26\n",
            "It: 37700, Loss: 4.526e-02, Loss_res: 5.784e-04,  Loss_bcs: 4.298e-02, Loss_ut_ics: 1.706e-03,, Time: 16.46\n",
            "It: 37800, Loss: 5.321e-02, Loss_res: 6.503e-04,  Loss_bcs: 5.057e-02, Loss_ut_ics: 1.989e-03,, Time: 19.44\n",
            "It: 37900, Loss: 4.754e-02, Loss_res: 7.892e-04,  Loss_bcs: 4.507e-02, Loss_ut_ics: 1.683e-03,, Time: 14.45\n",
            "It: 38000, Loss: 4.716e-02, Loss_res: 8.235e-04,  Loss_bcs: 4.469e-02, Loss_ut_ics: 1.645e-03,, Time: 18.54\n",
            "It: 38100, Loss: 4.981e-02, Loss_res: 8.016e-04,  Loss_bcs: 4.734e-02, Loss_ut_ics: 1.675e-03,, Time: 15.77\n",
            "It: 38200, Loss: 5.690e-02, Loss_res: 1.006e-03,  Loss_bcs: 5.358e-02, Loss_ut_ics: 2.319e-03,, Time: 17.63\n",
            "It: 38300, Loss: 5.112e-02, Loss_res: 7.139e-04,  Loss_bcs: 4.875e-02, Loss_ut_ics: 1.654e-03,, Time: 16.07\n",
            "It: 38400, Loss: 5.135e-02, Loss_res: 7.851e-04,  Loss_bcs: 4.888e-02, Loss_ut_ics: 1.678e-03,, Time: 16.10\n",
            "It: 38500, Loss: 4.628e-02, Loss_res: 7.991e-04,  Loss_bcs: 4.406e-02, Loss_ut_ics: 1.416e-03,, Time: 16.80\n",
            "It: 38600, Loss: 5.150e-02, Loss_res: 6.251e-04,  Loss_bcs: 4.939e-02, Loss_ut_ics: 1.482e-03,, Time: 15.06\n",
            "It: 38700, Loss: 4.708e-02, Loss_res: 1.138e-03,  Loss_bcs: 4.471e-02, Loss_ut_ics: 1.231e-03,, Time: 18.08\n",
            "It: 38800, Loss: 4.635e-02, Loss_res: 7.331e-04,  Loss_bcs: 4.413e-02, Loss_ut_ics: 1.492e-03,, Time: 13.79\n",
            "It: 38900, Loss: 4.787e-02, Loss_res: 1.079e-03,  Loss_bcs: 4.535e-02, Loss_ut_ics: 1.441e-03,, Time: 18.02\n",
            "It: 39000, Loss: 4.970e-02, Loss_res: 1.109e-03,  Loss_bcs: 4.730e-02, Loss_ut_ics: 1.293e-03,, Time: 14.64\n",
            "It: 39100, Loss: 4.846e-02, Loss_res: 1.114e-03,  Loss_bcs: 4.586e-02, Loss_ut_ics: 1.486e-03,, Time: 17.31\n",
            "It: 39200, Loss: 5.129e-02, Loss_res: 1.107e-03,  Loss_bcs: 4.893e-02, Loss_ut_ics: 1.256e-03,, Time: 15.82\n",
            "It: 39300, Loss: 4.848e-02, Loss_res: 1.855e-03,  Loss_bcs: 4.528e-02, Loss_ut_ics: 1.339e-03,, Time: 16.32\n",
            "It: 39400, Loss: 4.808e-02, Loss_res: 6.771e-04,  Loss_bcs: 4.630e-02, Loss_ut_ics: 1.102e-03,, Time: 16.58\n",
            "It: 39500, Loss: 4.290e-02, Loss_res: 8.885e-04,  Loss_bcs: 4.097e-02, Loss_ut_ics: 1.043e-03,, Time: 15.09\n",
            "It: 39600, Loss: 4.867e-02, Loss_res: 1.425e-03,  Loss_bcs: 4.587e-02, Loss_ut_ics: 1.382e-03,, Time: 17.94\n",
            "It: 39700, Loss: 4.889e-02, Loss_res: 7.345e-04,  Loss_bcs: 4.716e-02, Loss_ut_ics: 9.954e-04,, Time: 14.80\n",
            "It: 39800, Loss: 5.168e-02, Loss_res: 7.105e-04,  Loss_bcs: 4.961e-02, Loss_ut_ics: 1.358e-03,, Time: 18.16\n",
            "It: 39900, Loss: 5.040e-02, Loss_res: 1.001e-03,  Loss_bcs: 4.836e-02, Loss_ut_ics: 1.046e-03,, Time: 14.77\n",
            "elapsed: 4.33e+03\n",
            "Relative L2 error_u: 4.29e-01\n",
            "elapsed: 4.33e+03\n",
            "Relative L2 error_u: 4.29e-01\n",
            "\n",
            "\n",
            "Method:  mini_batch\n",
            "\n",
            "average of time_list: 4326.0688898563385\n",
            "average of error_u_list: 0.4293227294948388\n"
          ]
        }
      ],
      "source": [
        "# Define PINN model\n",
        "a = 0.5\n",
        "c = 2\n",
        "\n",
        "kernel_size = 300\n",
        "\n",
        "# Domain boundaries\n",
        "ics_coords = np.array([[0.0, 0.0],  [0.0, 1.0]])\n",
        "bc1_coords = np.array([[0.0, 0.0],  [1.0, 0.0]])\n",
        "bc2_coords = np.array([[0.0, 1.0],  [1.0, 1.0]])\n",
        "dom_coords = np.array([[0.0, 0.0],  [1.0, 1.0]])\n",
        "\n",
        "# Create initial conditions samplers\n",
        "ics_sampler = Sampler(2, ics_coords, lambda x: u(x, a, c), name='Initial Condition 1')\n",
        "\n",
        "# Create boundary conditions samplers\n",
        "bc1 = Sampler(2, bc1_coords, lambda x: u(x, a, c), name='Dirichlet BC1')\n",
        "bc2 = Sampler(2, bc2_coords, lambda x: u(x, a, c), name='Dirichlet BC2')\n",
        "bcs_sampler = [bc1, bc2]\n",
        "\n",
        "# Create residual sampler\n",
        "res_sampler = Sampler(2, dom_coords, lambda x: r(x, a, c), name='Forcing')\n",
        "\n",
        "\n",
        "\n",
        "nIter =40000\n",
        "bcbatch_size = 500\n",
        "ubatch_size = 5000\n",
        "mbbatch_size = 300\n",
        "\n",
        "\n",
        "\n",
        "# Define model\n",
        "mode = 'M1'\n",
        "layers = [2, 500, 500, 500, 1]\n",
        "\n",
        "\n",
        "nn = 200\n",
        "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
        "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
        "t, x = np.meshgrid(t, x)\n",
        "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
        "\n",
        "u_star = u(X_star, a,c)\n",
        "r_star = r(X_star, a, c)\n",
        "\n",
        "iterations = 1\n",
        "methods = [  \"mini_batch\"]\n",
        "\n",
        "result_dict =  dict((mtd, []) for mtd in methods)\n",
        "\n",
        "for mtd in methods:\n",
        "    print(\"Method: \", mtd)\n",
        "    time_list = []\n",
        "    error_u_list = []\n",
        "    \n",
        "    for index in range(iterations):\n",
        "\n",
        "        print(\"Epoch: \", str(index+1))\n",
        "\n",
        "        # Create residual sampler\n",
        "\n",
        "        [elapsed, error_u] = test_method(mtd , layers,  ics_sampler, bcs_sampler, res_sampler, c ,kernel_size , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size )\n",
        "\n",
        "\n",
        "        print('elapsed: {:.2e}'.format(elapsed))\n",
        "        print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "\n",
        "        time_list.append(elapsed)\n",
        "        error_u_list.append(error_u)\n",
        "\n",
        "    print(\"\\n\\nMethod: \", mtd)\n",
        "    print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
        "    print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
        "    # print(\"average of error_r_list:\" , sum(error_r_list) / len(error_r_list) )\n",
        "\n",
        "    result_dict[mtd] = [time_list ,error_u_list]\n",
        "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
        "\n",
        "    scipy.io.savemat(\"./1DWave_database/mini_batch_1Dwave_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_\"+str(iterations)+\".mat\" , result_dict)\n",
        "\n",
        "###############################################################################################################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZtWEM9-brXF",
        "outputId": "371feba5-fed5-41cb-9e3e-5c8497c4fbe6"
      },
      "outputs": [],
      "source": [
        "\n",
        "import scipy.io\n",
        "\n",
        "mode = 'M1'\n",
        "mbbatch_size = 128\n",
        "ubatch_size = 5000\n",
        "bcbatch_size = 500\n",
        "iterations = 40000\n",
        "\n",
        "time_list = []\n",
        "error_u_list = []\n",
        "error_v_list = []\n",
        "error_p_list = []\n",
        "    \n",
        "methods = [\"mini_batch\" , \"full_batch\"]\n",
        "result_dict =  dict((mtd, []) for mtd in methods)\n",
        "\n",
        "##Mini Batch\n",
        "time_list = [3124.91]\n",
        "error_u_list = [ 0.0041]\n",
        "\n",
        "\n",
        "result_dict[\"mini_batch\"] = [time_list ,error_u_list]\n",
        "\n",
        "print(\"\\n\\nMethod: \", mtd)\n",
        "print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
        "print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
        "\n",
        "##Full Batch\n",
        "time_list = []\n",
        "error_u_list = [ ]\n",
        "error_v_list = []\n",
        "error_p_list = []\n",
        "\n",
        "result_dict[\"full_batch\"] = [time_list ,error_u_list ,error_v_list ,  error_p_list]\n",
        "\n",
        "print(\"\\n\\nMethod: \", mtd)\n",
        "print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
        "print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
        "print(\"average of error_v_list:\" , sum(error_v_list) / len(error_v_list) )\n",
        "print(\"average of error_p_list:\" , sum(error_p_list) / len(error_p_list) )\n",
        "\n",
        "\n",
        "scipy.io.savemat(\"./1DWave_database/uhem_1DWave_model_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_\"+str(bcbatch_size)+\"_\"+str(iterations)+\".mat\" , result_dict)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vF1hwPUobyPE"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "itertaions = 80001\n",
        "log_NTK = True # Compute and store NTK matrix during training\n",
        "update_lam = True # Compute and update the loss weights using the NTK \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiyikOwBjRoZ"
      },
      "source": [
        "**Training Loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "Fw807UNzhu5z",
        "outputId": "f4551313-ffbc-49b5-8fd3-1296fc1641fe"
      },
      "outputs": [],
      "source": [
        "loss_res = model.loss_res_log\n",
        "loss_bcs = model.loss_bcs_log\n",
        "loss_u_t_ics = model.loss_ut_ics_log\n",
        "\n",
        "fig = plt.figure(figsize=(6, 5))\n",
        "plt.plot(loss_res, label='$\\mathcal{L}_{r}$')\n",
        "plt.plot(loss_bcs, label='$\\mathcal{L}_{u}$')\n",
        "plt.plot(loss_u_t_ics, label='$\\mathcal{L}_{u_t}$')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFLIBq5xjZ3v"
      },
      "source": [
        "**Model Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To0PDN17cc0v",
        "outputId": "1f47f288-322a-46b5-f173-45485191a68d"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Predictions\n",
        "u_pred = model.predict_u(X_star)\n",
        "r_pred = model.predict_r(X_star)\n",
        "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "\n",
        "print('Relative L2 error_u: %e' % (error_u))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "K428lOuXhdc8",
        "outputId": "015f591b-d8a4-4e47-8020-84fcf219d7ca"
      },
      "outputs": [],
      "source": [
        "U_star = griddata(X_star, u_star.flatten(), (t, x), method='cubic')\n",
        "r_star = griddata(X_star, r_star.flatten(), (t, x), method='cubic')\n",
        "U_pred = griddata(X_star, u_pred.flatten(), (t, x), method='cubic')\n",
        "R_pred = griddata(X_star, r_pred.flatten(), (t, x), method='cubic')\n",
        "\n",
        "\n",
        "plt.figure(figsize=(18, 9))\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.pcolor(t, x, U_star, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "plt.title('Exact u(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.pcolor(t, x, U_pred, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Predicted u(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.pcolor(t, x, np.abs(U_star - U_pred), cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Absolute error')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.pcolor(t, x, r_star, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Exact r(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.pcolor(t, x, R_pred, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Predicted r(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 6)\n",
        "plt.pcolor(t, x, np.abs(r_star - R_pred), cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Absolute error')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EYdfKGLj6h0"
      },
      "source": [
        "**NTK Eigenvalues**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3dByeQjhBYj"
      },
      "outputs": [],
      "source": [
        "# Create empty lists for storing the eigenvalues of NTK\n",
        "lam_K_u_log = []\n",
        "lam_K_ut_log = []\n",
        "lam_K_r_log = []\n",
        "\n",
        "# Restore the NTK\n",
        "K_u_list = model.K_u_log\n",
        "K_ut_list = model.K_ut_log\n",
        "K_r_list = model.K_r_log\n",
        "\n",
        "K_list = []\n",
        "    \n",
        "for k in range(len(K_u_list)):\n",
        "    K_u = K_u_list[k]\n",
        "    K_ut = K_ut_list[k]\n",
        "    K_r = K_r_list[k]\n",
        "    \n",
        "    # Compute eigenvalues\n",
        "    lam_K_u, _ = np.linalg.eig(K_u)\n",
        "    lam_K_ut, _ = np.linalg.eig(K_ut)\n",
        "    lam_K_r, _ = np.linalg.eig(K_r)\n",
        "    # Sort in descresing order\n",
        "    lam_K_u = np.sort(np.real(lam_K_u))[::-1]\n",
        "    lam_K_ut = np.sort(np.real(lam_K_ut))[::-1]\n",
        "    lam_K_r = np.sort(np.real(lam_K_r))[::-1]\n",
        "    \n",
        "    # Store eigenvalues\n",
        "    lam_K_u_log.append(lam_K_u)\n",
        "    lam_K_ut_log.append(lam_K_ut)\n",
        "    lam_K_r_log.append(lam_K_r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "vSn3Q_1IhisN",
        "outputId": "886908b3-c316-48d6-933f-81b1180ff954"
      },
      "outputs": [],
      "source": [
        "#  Eigenvalues of NTK\n",
        "fig = plt.figure(figsize=(18, 5))\n",
        "plt.subplot(1,3,1)\n",
        "\n",
        "plt.plot(lam_K_u_log[0], label = '$n=0$')\n",
        "plt.plot(lam_K_u_log[1], '--', label = '$n=10,000$')\n",
        "plt.plot(lam_K_u_log[4], '--', label = '$n=40,000$')\n",
        "plt.plot(lam_K_u_log[-1], '--', label = '$n=80,000$')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.title(r'Eigenvalues of ${K}_u$')\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.plot(lam_K_ut_log[0], label = '$n=0$')\n",
        "plt.plot(lam_K_ut_log[1], '--',label = '$n=10,000$')\n",
        "plt.plot(lam_K_ut_log[4], '--', label = '$n=40,000$')\n",
        "plt.plot(lam_K_ut_log[-1], '--', label = '$n=80,000$')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.title(r'Eigenvalues of ${K}_{u_t}$')\n",
        "\n",
        "ax =plt.subplot(1,3,3)\n",
        "plt.plot(lam_K_r_log[0], label = '$n=0$')\n",
        "plt.plot(lam_K_r_log[1], '--', label = '$n=10,000$')\n",
        "plt.plot(lam_K_r_log[4], '--', label = '$n=40,000$')\n",
        "plt.plot(lam_K_r_log[-1], '--', label = '$n=80,000$')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.title(r'Eigenvalues of ${K}_{r}$')\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "fig.legend(handles, labels, loc=\"upper left\", bbox_to_anchor=(0.35, -0.02),\n",
        "            borderaxespad=0, bbox_transform=fig.transFigure, ncol=4)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbUn_fcowojl"
      },
      "source": [
        "**Evolution of NTK Weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYbzkhfMjJ8k"
      },
      "outputs": [],
      "source": [
        "if update_lam == True:\n",
        "\n",
        "  lam_u_log = model.lam_u_log\n",
        "  lam_ut_log = model.lam_ut_log\n",
        "  lam_r_log = model.lam_r_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "xzFzPCA2w1ML",
        "outputId": "71452cf9-3ebb-4aeb-9708-c7664b88e65d"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6, 5))\n",
        "plt.plot(lam_u_log, label='$\\lambda_u$')\n",
        "plt.plot(lam_ut_log, label='$\\lambda_{u_t}$')\n",
        "plt.plot(lam_r_log, label='$\\lambda_{r}$')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('$\\lambda$')\n",
        "plt.yscale('log')\n",
        "plt.legend( )\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mimIv2Z5xlip"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PINNsNTK_Wave.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
